<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-04-07T00:57:57+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sometimes Models Just Do Things</title><subtitle>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</subtitle><entry><title type="html">Google’s Second Chance at AI Dominance</title><link href="http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html" rel="alternate" type="text/html" title="Google’s Second Chance at AI Dominance" /><published>2025-04-06T15:00:00+01:00</published><updated>2025-04-06T15:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html"><![CDATA[<p><img src="/assets/images/posts/458D04C0-8CE5-4CA5-B088-071F1E171B5E_4_5005_c.jpeg" alt="Google AI" class="align-center" style="max-width: 100%; margin: 20px auto;" /></p>

<p>One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book.</p>

<p>Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023.</p>

<h2 id="the-business-of-benchmarks">The Business of Benchmarks</h2>

<p>It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush.</p>

<p>Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access.</p>

<h2 id="the-multimodal-moat">The Multimodal Moat</h2>

<p>Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system.</p>

<p>The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<h2 id="the-vc-to-viability-pipeline">The VC-to-Viability Pipeline</h2>

<p>For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen.</p>

<p>This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p>

<h2 id="pricing-the-great-unknown">Pricing: The Great Unknown</h2>

<p>The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost.</p>

<p>If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem.</p>

<h2 id="the-future-of-ai-thinking">The Future of AI Thinking</h2>

<p>Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio.</p>

<p>The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus.</p>

<p>Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales.</p>

<p>But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book. Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.1 This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023. The Business of Benchmarks It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.2 The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush. Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access. The Multimodal Moat Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system. The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.3 The VC-to-Viability Pipeline For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen. This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.4 Pricing: The Great Unknown The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost. If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem. The Future of AI Thinking Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio. The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus. Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales. But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle. Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. &#8617; There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. &#8617; This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” &#8617; For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. &#8617;]]></summary></entry><entry><title type="html">AI, Insight, and the Business of Science</title><link href="http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html" rel="alternate" type="text/html" title="AI, Insight, and the Business of Science" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html"><![CDATA[<p><img src="/assets/images/posts/4D3740B3-9D63-45A7-95A8-340E70094870.png" alt="AI and Science" class="align-center" style="max-width: 100%;" /></p>

<p>One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas.</p>

<p>This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape.</p>

<h2 id="the-industrial-processing-of-information">The industrial processing of information</h2>

<p>AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports).</p>

<p>This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable.</p>

<p>But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems.</p>

<p>This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries.</p>

<h2 id="big-science-and-the-acceleration-paradox">Big Science™ and the acceleration paradox</h2>

<p>The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand.</p>

<p>These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones.</p>

<p>Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours.</p>

<p>This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed.</p>

<p>The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers.</p>

<h2 id="the-institutional-challenge">The institutional challenge</h2>

<p>Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints.</p>

<p>Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically.</p>

<p>Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation.</p>

<p>This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers.</p>

<p>The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research.</p>

<h2 id="information-vs-insight-revisited">Information vs. insight revisited</h2>

<p>This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish.</p>

<p>This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage.</p>

<p>The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation.</p>

<p>And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide.</p>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas. This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape. The industrial processing of information AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports). This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable. But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems. This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries. Big Science™ and the acceleration paradox The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand. These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones. Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours. This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed. The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers. The institutional challenge Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints. Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically. Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation. This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers. The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research. Information vs. insight revisited This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish. This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage. The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation. And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide.]]></summary></entry><entry><title type="html">Comparing Credit Rating Systems: Deep Research vs. DeepCredit v0.1</title><link href="http://localhost:4000/experiments/2025/04/06/comparing-credit-rating-systems.html" rel="alternate" type="text/html" title="Comparing Credit Rating Systems: Deep Research vs. DeepCredit v0.1" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/experiments/2025/04/06/comparing-credit-rating-systems</id><content type="html" xml:base="http://localhost:4000/experiments/2025/04/06/comparing-credit-rating-systems.html"><![CDATA[<p><img src="/assets/images/posts/5B53E370-ED59-4FC0-930C-850D5B2161AB.png" alt="Credit Rating Systems Comparison" class="align-center" style="max-width: 100%;" /></p>

<h2 id="experiment-overview">Experiment Overview</h2>

<p>I recently conducted an experiment comparing OpenAI’s Deep Research feature against the first version of my custom-built “DeepCredit” agent for generating credit rating reports. As part of the Data and Analytics team working to advance our AI capabilities, I wanted to benchmark these two approaches to understand their relative strengths and potential for credit analysis.</p>

<p>The experiment focused on generating comprehensive credit rating reports for Trafigura Group’s senior unsecured long-term debt using publicly available information. Both systems produced complete reports that were then evaluated by an independent OpenAI model (“o1”) against standardized criteria.</p>

<h2 id="agent-architecture-and-capabilities">Agent Architecture and Capabilities</h2>

<h3 id="deepcredit-v01">DeepCredit v0.1</h3>

<p>My DeepCredit agent was built using the OpenAI Agents SDK with a multi-agent architecture. The system employs an orchestrated workflow where specialized agents work together iteratively:</p>

<ol>
  <li><strong>Knowledge Gap Agent</strong>: Analyzes current research state and identifies gaps</li>
  <li><strong>Tool Selector Agent</strong>: Determines which tools to use for addressing specific gaps</li>
  <li><strong>Tool Agents</strong>: Specialized for executing research actions:
    <ul>
      <li>Web Search Agent</li>
      <li>Website Crawler Agent</li>
    </ul>
  </li>
  <li><strong>Writer Agent</strong>: Synthesizes findings into a coherent report</li>
</ol>

<p>In its current v0.1 state, DeepCredit can only scrape websites for information, with limited RAG capabilities and no PDF parsing functionality.</p>

<h3 id="openai-deep-research">OpenAI Deep Research</h3>

<p>OpenAI’s Deep Research is a more mature implementation with enhanced capabilities:</p>

<ul>
  <li>Trained using reinforcement fine-tuning</li>
  <li>Equipped with browser access for real-time information gathering</li>
  <li>Closed system with no visibility into its underlying architecture</li>
</ul>

<h2 id="evaluation-methodology">Evaluation Methodology</h2>

<p>Both systems were tasked with the same assignment: produce a comprehensive credit rating report for Trafigura Group. The resulting reports were then evaluated against three key criteria:</p>

<ol>
  <li><strong>Quality of Information</strong> (35 points): Breadth and depth of data, clarity of financial metrics</li>
  <li><strong>Quality of the Report</strong> (35 points): Organization, thoroughness, overall presentation</li>
  <li><strong>Justification of the Final Rating</strong> (30 points): How convincingly the report explains the assigned rating</li>
</ol>

<h2 id="results-analysis">Results Analysis</h2>

<h3 id="report-structure-and-scope">Report Structure and Scope</h3>

<p><strong>DeepCredit v0.1</strong> produced a well-organized report with six key segments:</p>

<ul>
  <li>Methodology &amp; Rationale</li>
  <li>Data Gathering &amp; Analysis</li>
  <li>Risk &amp; Creditworthiness</li>
  <li>Information Gaps</li>
  <li>Final Credit Rating</li>
  <li>Detailed Report Structure</li>
</ul>

<p>The report stayed at a moderate level of detail, with concise summaries and a numbered reference list for citations.</p>

<p><strong>OpenAI Deep Research</strong> delivered a comprehensive narrative that closely mirrored professional rating agency reports. It provided detailed explanation of methodologies, granular financial metrics, thorough risk assessment, and extensive rating justification with industry benchmarking.</p>

<h3 id="information-quality">Information Quality</h3>

<p><strong>DeepCredit v0.1</strong> (scored 25/35):</p>

<ul>
  <li>Cited key high-level figures (e.g., adjusted debt-to-equity ratio of -0.43x)</li>
  <li>Referenced fraud losses and legal issues</li>
  <li>Provided general information on liquidity and credit facilities</li>
  <li>Lacked granular ratio calculations and detailed analysis</li>
</ul>

<p><strong>OpenAI Deep Research</strong> (scored 33/35):</p>

<ul>
  <li>Incorporated detailed metrics (e.g., total debt breakdown, revenue trends from $318B to $244B)</li>
  <li>Calculated key financial ratios (debt-to-EBITDA, EBITDA/interest coverage)</li>
  <li>Drew explicit comparisons to industry peers like Glencore and Bunge</li>
  <li>Expanded on ESG factors, governance, compliance issues, and event risks</li>
  <li>Provided historical context and forward-looking analysis</li>
</ul>

<h3 id="report-quality">Report Quality</h3>

<p><strong>DeepCredit v0.1</strong> (scored 25/35):</p>

<ul>
  <li>Clear structure with well-organized sections</li>
  <li>Suitable for quick reading with bullet points</li>
  <li>Remained somewhat templated and outline-level</li>
  <li>“Detailed Report Structure” section was more of a skeleton than a complete analysis</li>
</ul>

<p><strong>OpenAI Deep Research</strong> (scored 33/35):</p>

<ul>
  <li>Logical narrative flow from methodology to final conclusion</li>
  <li>Extensive use of subheadings and bullet points</li>
  <li>Effectively tied data to rating rationale</li>
  <li>Thorough explanation of each dimension of credit risk</li>
  <li>Professional-quality analysis similar to formal rating agency reports</li>
</ul>

<h3 id="rating-justification">Rating Justification</h3>

<p>Both systems assigned Trafigura a <strong>BBB+ rating with a stable outlook</strong>, with similar core justifications:</p>

<ul>
  <li>Strong liquidity via large credit facilities</li>
  <li>Sizable equity base and ongoing profitability</li>
  <li>Recent fraud losses and legal issues as manageable concerns</li>
  <li>Inherent volatility in commodity trading</li>
</ul>

<p>However, the depth of justification differed significantly:</p>

<p><strong>DeepCredit v0.1</strong> (scored 20/30):</p>

<ul>
  <li>Succinct explanation with high-level bullet points</li>
  <li>Brief mentions of strengths and weaknesses</li>
  <li>Limited detail on how specific metrics supported the rating</li>
</ul>

<p><strong>OpenAI Deep Research</strong> (scored 27/30):</p>

<ul>
  <li>Multiple paragraphs of detailed rating rationale</li>
  <li>Benchmarked against known rating agency frameworks</li>
  <li>Explained how specific financial metrics translated into the BBB+ rating</li>
  <li>Provided in-depth reasoning for why the rating wasn’t higher or lower</li>
</ul>

<h3 id="overall-scoring">Overall Scoring</h3>

<p><strong>DeepCredit v0.1</strong>: ~70-75/100<br />
<strong>OpenAI Deep Research</strong>: ~93/100</p>

<h2 id="key-findings-and-implications">Key Findings and Implications</h2>

<p>The experiment clearly demonstrated that OpenAI’s Deep Research feature significantly outperformed DeepCredit v0.1. This outcome, while not unexpected given the relative maturity of the systems, raises important questions about the current state and future potential of AI in credit analysis:</p>

<ol>
  <li><strong>Information Depth is Critical</strong>: The ability to gather, synthesize, and present detailed financial data and contextual information dramatically enhances the credibility of credit assessments.</li>
  <li><strong>Narrative Structure Matters</strong>: The OpenAI Deep Research report’s cohesive narrative flow made complex financial information more accessible and convincing compared to the more segmented approach of DeepCredit.</li>
  <li><strong>Comparative Analysis Adds Value</strong>: Explicit benchmarking against industry peers and rating frameworks significantly strengthens rating justifications.</li>
  <li><strong>Potential for AI in Credit Analysis</strong>: The quality of the OpenAI Deep Research report suggests that with proper prompt engineering, AI systems may already be approaching reliability for preliminary credit assessments.</li>
</ol>

<h2 id="next-research-steps">Next Research Steps</h2>

<p>Based on these findings, I’ve identified several promising avenues for advancing our AI-powered credit analysis capabilities:</p>

<ol>
  <li><strong>Explore OpenAI Deep Research’s Full Potential</strong>: While Deep Research is a closed product that can’t be modified internally, we can experiment with more sophisticated prompting strategies to optimize its credit analysis capabilities.</li>
  <li><strong>Establish Robust Evaluation Framework</strong>: Set up comprehensive evaluation protocols using OpenAI Deep Research and actual credit rating agency reports as benchmarks, with an LLM judge comparing them against standardized scoring criteria.</li>
  <li><strong>Enhance DeepCredit’s Tooling</strong>: Add PDF parsing and code interpreter capabilities to enable DeepCredit to work with a broader range of financial documents and perform quantitative analysis.</li>
  <li><strong>Refine Agent System Prompts</strong>: Systematically improve the prompts for each agent in the DeepCredit architecture to measure incremental performance gains.</li>
  <li><strong>Experiment with Fine-Tuning</strong>: Begin testing fine-tuning approaches for individual agents using existing tools to improve specialized performance.</li>
  <li><strong>Explore End-to-End Fine-Tuning</strong>: Request research access to OpenAI’s Reinforcement Fine-Tuning API (which was used to train OpenAI’s Deep Research) to potentially implement end-to-end fine-tuning of the entire agentic workflow.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>This experiment provides valuable insights into the current state and future potential of AI-powered credit analysis. While DeepCredit v0.1 demonstrates promise as a structured approach to credit research, the superior performance of OpenAI’s Deep Research highlights both the challenges ahead and the considerable potential for AI in this domain.</p>

<p>The next phase of research will focus on narrowing this performance gap by enhancing DeepCredit’s capabilities and exploring more sophisticated prompting strategies for Deep Research. The end goal remains developing reliable AI-powered credit analysis tools that can support our organization’s financial decision-making processes with comprehensive, well-reasoned assessments.</p>]]></content><author><name>Ben Reeve</name></author><category term="experiments" /><summary type="html"><![CDATA[Experiment Overview I recently conducted an experiment comparing OpenAI’s Deep Research feature against the first version of my custom-built “DeepCredit” agent for generating credit rating reports. As part of the Data and Analytics team working to advance our AI capabilities, I wanted to benchmark these two approaches to understand their relative strengths and potential for credit analysis. The experiment focused on generating comprehensive credit rating reports for Trafigura Group’s senior unsecured long-term debt using publicly available information. Both systems produced complete reports that were then evaluated by an independent OpenAI model (“o1”) against standardized criteria. Agent Architecture and Capabilities DeepCredit v0.1 My DeepCredit agent was built using the OpenAI Agents SDK with a multi-agent architecture. The system employs an orchestrated workflow where specialized agents work together iteratively: Knowledge Gap Agent: Analyzes current research state and identifies gaps Tool Selector Agent: Determines which tools to use for addressing specific gaps Tool Agents: Specialized for executing research actions: Web Search Agent Website Crawler Agent Writer Agent: Synthesizes findings into a coherent report In its current v0.1 state, DeepCredit can only scrape websites for information, with limited RAG capabilities and no PDF parsing functionality. OpenAI Deep Research OpenAI’s Deep Research is a more mature implementation with enhanced capabilities: Trained using reinforcement fine-tuning Equipped with browser access for real-time information gathering Closed system with no visibility into its underlying architecture Evaluation Methodology Both systems were tasked with the same assignment: produce a comprehensive credit rating report for Trafigura Group. The resulting reports were then evaluated against three key criteria: Quality of Information (35 points): Breadth and depth of data, clarity of financial metrics Quality of the Report (35 points): Organization, thoroughness, overall presentation Justification of the Final Rating (30 points): How convincingly the report explains the assigned rating Results Analysis Report Structure and Scope DeepCredit v0.1 produced a well-organized report with six key segments: Methodology &amp; Rationale Data Gathering &amp; Analysis Risk &amp; Creditworthiness Information Gaps Final Credit Rating Detailed Report Structure The report stayed at a moderate level of detail, with concise summaries and a numbered reference list for citations. OpenAI Deep Research delivered a comprehensive narrative that closely mirrored professional rating agency reports. It provided detailed explanation of methodologies, granular financial metrics, thorough risk assessment, and extensive rating justification with industry benchmarking. Information Quality DeepCredit v0.1 (scored 25/35): Cited key high-level figures (e.g., adjusted debt-to-equity ratio of -0.43x) Referenced fraud losses and legal issues Provided general information on liquidity and credit facilities Lacked granular ratio calculations and detailed analysis OpenAI Deep Research (scored 33/35): Incorporated detailed metrics (e.g., total debt breakdown, revenue trends from $318B to $244B) Calculated key financial ratios (debt-to-EBITDA, EBITDA/interest coverage) Drew explicit comparisons to industry peers like Glencore and Bunge Expanded on ESG factors, governance, compliance issues, and event risks Provided historical context and forward-looking analysis Report Quality DeepCredit v0.1 (scored 25/35): Clear structure with well-organized sections Suitable for quick reading with bullet points Remained somewhat templated and outline-level “Detailed Report Structure” section was more of a skeleton than a complete analysis OpenAI Deep Research (scored 33/35): Logical narrative flow from methodology to final conclusion Extensive use of subheadings and bullet points Effectively tied data to rating rationale Thorough explanation of each dimension of credit risk Professional-quality analysis similar to formal rating agency reports Rating Justification Both systems assigned Trafigura a BBB+ rating with a stable outlook, with similar core justifications: Strong liquidity via large credit facilities Sizable equity base and ongoing profitability Recent fraud losses and legal issues as manageable concerns Inherent volatility in commodity trading However, the depth of justification differed significantly: DeepCredit v0.1 (scored 20/30): Succinct explanation with high-level bullet points Brief mentions of strengths and weaknesses Limited detail on how specific metrics supported the rating OpenAI Deep Research (scored 27/30): Multiple paragraphs of detailed rating rationale Benchmarked against known rating agency frameworks Explained how specific financial metrics translated into the BBB+ rating Provided in-depth reasoning for why the rating wasn’t higher or lower Overall Scoring DeepCredit v0.1: ~70-75/100 OpenAI Deep Research: ~93/100 Key Findings and Implications The experiment clearly demonstrated that OpenAI’s Deep Research feature significantly outperformed DeepCredit v0.1. This outcome, while not unexpected given the relative maturity of the systems, raises important questions about the current state and future potential of AI in credit analysis: Information Depth is Critical: The ability to gather, synthesize, and present detailed financial data and contextual information dramatically enhances the credibility of credit assessments. Narrative Structure Matters: The OpenAI Deep Research report’s cohesive narrative flow made complex financial information more accessible and convincing compared to the more segmented approach of DeepCredit. Comparative Analysis Adds Value: Explicit benchmarking against industry peers and rating frameworks significantly strengthens rating justifications. Potential for AI in Credit Analysis: The quality of the OpenAI Deep Research report suggests that with proper prompt engineering, AI systems may already be approaching reliability for preliminary credit assessments. Next Research Steps Based on these findings, I’ve identified several promising avenues for advancing our AI-powered credit analysis capabilities: Explore OpenAI Deep Research’s Full Potential: While Deep Research is a closed product that can’t be modified internally, we can experiment with more sophisticated prompting strategies to optimize its credit analysis capabilities. Establish Robust Evaluation Framework: Set up comprehensive evaluation protocols using OpenAI Deep Research and actual credit rating agency reports as benchmarks, with an LLM judge comparing them against standardized scoring criteria. Enhance DeepCredit’s Tooling: Add PDF parsing and code interpreter capabilities to enable DeepCredit to work with a broader range of financial documents and perform quantitative analysis. Refine Agent System Prompts: Systematically improve the prompts for each agent in the DeepCredit architecture to measure incremental performance gains. Experiment with Fine-Tuning: Begin testing fine-tuning approaches for individual agents using existing tools to improve specialized performance. Explore End-to-End Fine-Tuning: Request research access to OpenAI’s Reinforcement Fine-Tuning API (which was used to train OpenAI’s Deep Research) to potentially implement end-to-end fine-tuning of the entire agentic workflow. Conclusion This experiment provides valuable insights into the current state and future potential of AI-powered credit analysis. While DeepCredit v0.1 demonstrates promise as a structured approach to credit research, the superior performance of OpenAI’s Deep Research highlights both the challenges ahead and the considerable potential for AI in this domain. The next phase of research will focus on narrowing this performance gap by enhancing DeepCredit’s capabilities and exploring more sophisticated prompting strategies for Deep Research. The end goal remains developing reliable AI-powered credit analysis tools that can support our organization’s financial decision-making processes with comprehensive, well-reasoned assessments.]]></summary></entry><entry><title type="html">The Deep Dive: A Superuser Guide to OpenAI Deep Research</title><link href="http://localhost:4000/tutorials/2025/04/06/deep-research-superuser-guide.html" rel="alternate" type="text/html" title="The Deep Dive: A Superuser Guide to OpenAI Deep Research" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/tutorials/2025/04/06/deep-research-superuser-guide</id><content type="html" xml:base="http://localhost:4000/tutorials/2025/04/06/deep-research-superuser-guide.html"><![CDATA[<p><img src="/assets/images/posts/8FB965CC-9D0C-4159-B9CE-8B81907EAB02.png" alt="OpenAI Deep Research" class="align-center" style="max-width: 100%;" /></p>

<h2 id="welcome-to-the-knowledge-navigators-club">Welcome to the Knowledge Navigators Club!</h2>

<p>Hello brilliant minds of our Knowledge Services team! You’ve just been granted access to what might be the most powerful research assistant since the invention of the internet itself: OpenAI’s Deep Research. This isn’t your garden-variety AI—it’s a research powerhouse that can dive into the depths of the web, synthesize information from hundreds of sources, and deliver comprehensive, analyst-grade reports that would normally take us mere mortals hours (or days) to compile.</p>

<p>But with great power comes great responsibility—and a monthly limit of 120 queries. So let’s make every Deep Research adventure count!</p>

<blockquote>
  <p><strong>⚠️ CRITICAL REMINDER</strong>: Deep Research connects to the live internet. Never, ever input confidential client information, internal data, or sensitive material into your prompts. Keep your queries focused on publicly available information only. What happens in our Knowledge Management system stays in our Knowledge Management system!</p>
</blockquote>

<h2 id="why-deep-research-is-your-new-best-friend">Why Deep Research Is Your New Best Friend</h2>

<p>Before we dive into the prompting wizardry, let’s understand what makes Deep Research special. It was trained using Reinforcement Learning on “hard browsing and reasoning tasks,” which is fancy AI-speak for “this thing knows how to research like a pro.”</p>

<p>Its superpower is dynamic adaptation—it adjusts its research strategy as it goes, deciding what to search for next based on what it just learned. Think of it as the difference between following a rigid research plan and having a curious, intelligent assistant who can pivot when they discover something interesting or unexpected.</p>

<p>While it’s pretty good at figuring things out on its own, upfront guidance in your prompt helps tremendously at those critical decision points. Consider your prompt as setting the GPS coordinates before the AI embarks on its research journey—the clearer your directions, the more targeted the destination.</p>

<h2 id="the-art-of-the-perfect-prompt">The Art of the Perfect Prompt</h2>

<h3 id="1-structure-is-your-friend">1. Structure is Your Friend</h3>

<p>Deep Research responds beautifully to structured prompts that clearly define what you want. Think of your prompt as a research brief you’d give to a junior analyst.</p>

<p><strong>Template: The Basic Research Brief</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I need comprehensive research on [TOPIC]. Please focus on:
- [ASPECT 1] from [TIMEFRAME]
- [ASPECT 2] with emphasis on [SPECIFIC ANGLE]
- [ASPECT 3] including [SPECIFIC DATA TYPE]

Prioritize authoritative sources such as academic journals, government publications, and industry reports. For financial data, favor company financial reports and regulatory filings over consulting firm marketing materials or opinion pieces.

Structure the report with an executive summary, key findings, and detailed analysis. Include relevant statistics and cite all sources.
</code></pre></div></div>

<p><strong>Real Example:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I need comprehensive research on autonomous vehicle regulations. Please focus on:
- Current regulatory frameworks in the US, EU, and China from 2020-present
- Safety standards with emphasis on liability issues
- Market adoption rates including quarterly growth statistics

Prioritize authoritative sources such as government regulatory documents, transportation department publications, and official industry standards. Avoid relying on consulting firm marketing materials or speculative blog posts.

Structure the report with an executive summary, key findings, and detailed analysis. Include relevant statistics and cite all sources.
</code></pre></div></div>

<h3 id="2-the-secret-sauce-clarity-detail-and-context">2. The Secret Sauce: Clarity, Detail, and Context</h3>

<p>Ambiguity is the enemy of good research. The more specific you are about scope, focus, and format, the better your results will be. Deep Research will ask clarifying questions if your prompt is too vague (which is actually helpful!), but you’ll save time by being detailed upfront.</p>

<p><strong>Template: The Detail-Rich Brief</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Analyze [SPECIFIC TOPIC] within [SPECIFIC PARAMETERS]. I'm specifically interested in:

1. [KEY QUESTION 1]?
2. [KEY QUESTION 2]?
3. [KEY QUESTION 3]?

For context, this research will be used for [PURPOSE], so please emphasize [RELEVANT ASPECTS] in your analysis. Include [SPECIFIC DATA TYPE] if available, and present the information in [PREFERRED FORMAT].

When researching, please prioritize [PREFERRED SOURCE TYPES] such as [EXAMPLES], and minimize reliance on [SOURCES TO AVOID].
</code></pre></div></div>

<p><strong>Real Example:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Analyze the impact of ESG investing on financial returns within public equity markets from 2018-2023. I'm specifically interested in:

1. How do ESG-focused funds compare to traditional funds in terms of risk-adjusted returns?
2. What sectors show the strongest correlation between ESG ratings and financial performance?
3. How have investor attitudes toward ESG changed since the pandemic?

For context, this research will be used for a client presentation to financial advisors, so please emphasize practical implications in your analysis. Include quantitative performance data if available, and present the information in a structured format with clear sections for each question.

When researching, please prioritize SEC filings, fund prospectuses, academic finance journals, and financial market data from established providers such as Bloomberg or Morningstar, and minimize reliance on asset management marketing materials or opinion pieces without supporting data.
</code></pre></div></div>

<h3 id="3-the-clarification-dance">3. The Clarification Dance</h3>

<p>When you submit your prompt, Deep Research will often respond with a brief clarification conversation before diving in. This isn’t a bug—it’s a feature! Treat this as an opportunity to refine your research parameters.</p>

<p><strong>Tips for Effective Clarifications:</strong></p>

<ul>
  <li>Answer clarifying questions thoroughly</li>
  <li>Use this opportunity to add any details you forgot in your initial prompt</li>
  <li>Be decisive when presented with options (global vs. regional focus, etc.)</li>
  <li>Confirm or correct the AI’s understanding of your query</li>
  <li>Reinforce source quality requirements if needed</li>
</ul>

<p><strong>Pro Tip</strong>: Sometimes you can anticipate common clarification questions and address them proactively in your initial prompt. For example: “Please take a global perspective unless specified otherwise” or “Focus on peer-reviewed studies rather than news articles when possible.”</p>

<h3 id="4-unleashing-the-reasoning-beast">4. Unleashing the Reasoning Beast</h3>

<p>Deep Research uses what they call an “o3 reasoning model,” which basically means it thinks deeply before answering. Once you’ve set up a good prompt and addressed clarifications, let it cook!</p>

<p>The model may take 5-30 minutes to compile a report, during which it’s searching, reading, and synthesizing information. Don’t interrupt it—patience yields better results. The model is doing internal chain-of-thought reasoning that leads to deeper analysis.</p>

<p><strong>Template: The Reasoning Enhancer</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For this research on [TOPIC], I'd like you to:

1. First outline your approach to answering this question
2. Gather information from diverse sources, including [SUGGESTED SOURCE TYPES]
3. Consider multiple perspectives, especially [PERSPECTIVE 1] and [PERSPECTIVE 2]
4. Synthesize the findings into a cohesive analysis
5. Provide recommendations based on the evidence

Prioritize primary sources including [SPECIFIC SOURCE TYPES] and be skeptical of [SOURCES TO TREAT WITH CAUTION]. When evaluating sources, favor those with robust methodologies and transparent data over opinion-based content.
</code></pre></div></div>

<h3 id="the-technology-trend-brief">The Technology Trend Brief</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Research the current state and future trajectory of [TECHNOLOGY] with focus on:

1. Stage of development and technological maturity
2. Key innovations and breakthroughs in the past [TIMEFRAME]
3. Major companies and research institutions advancing the technology
4. Practical applications and use cases, both current and potential
5. Regulatory considerations and ethical implications
6. Market adoption forecasts and barriers

Include a timeline of significant developments and cite reputable technical sources.

Please prioritize these source types:
- Academic journals and conference proceedings
- Patent filings and technical documentation
- Research institution publications
- Company technical whitepapers (not marketing materials)
- Regulatory and standards body publications

Please minimize reliance on:
- General media articles without technical depth
- Vendor marketing materials without technical substantiation
- Predictions without methodology explanations
</code></pre></div></div>

<h3 id="the-regulatory-landscape-brief">The Regulatory Landscape Brief</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Analyze the regulatory environment for [INDUSTRY/ACTIVITY] across [GEOGRAPHIC SCOPE], including:

1. Current regulatory frameworks and governing bodies
2. Recent significant regulatory changes
3. Pending legislation or regulatory proposals
4. Compliance requirements and challenges
5. International variations and harmonization efforts
6. Expert opinions on regulatory trends and future direction

For this analysis, please strongly prioritize:
- Official government and regulatory body publications
- Legislative texts and court decisions
- Regulatory agency guidance documents and enforcement actions
- Bar association or legal journal analyses by practicing attorneys
- Official international organization publications (e.g., OECD, WTO)

Please avoid or minimize reliance on:
- Non-specialist news summaries
- Company marketing materials about regulatory compliance
- Opinion pieces without legal expertise
- Outdated regulatory information
</code></pre></div></div>

<h2 id="deep-research-hacks-insider-tips-and-tricks">Deep Research Hacks: Insider Tips and Tricks</h2>

<h3 id="1-the-meta-prompt-strategy">1. The Meta-Prompt Strategy</h3>

<p>Use regular GPT-4 to help craft your Deep Research prompt before using one of your precious 120 monthly queries. This “prompt about a prompt” approach helps refine your query for maximum effectiveness.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I want to use OpenAI's Deep Research to gather information about [TOPIC]. Help me craft an optimal prompt that will:
1. Clearly define the scope and parameters
2. Specify the most relevant aspects to research
3. Include any necessary context
4. Structure the request for maximum clarity
5. Specify high-quality sources to prioritize and low-quality sources to avoid

The resulting prompt should be comprehensive yet focused, as I want to make the most of this research query.
</code></pre></div></div>

<h3 id="2-the-socratic-approach">2. The Socratic Approach</h3>

<p>For complex topics, consider framing your prompt as a series of nested questions that build upon each other.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Research [MAIN TOPIC] by exploring these interconnected questions:

1. [FOUNDATIONAL QUESTION]?
   a. How does this relate to [SUB-QUESTION 1a]?
   b. What are the implications for [SUB-QUESTION 1b]?

2. [BUILDING QUESTION]?
   a. What evidence supports or contradicts [SUB-QUESTION 2a]?
   b. How has this changed over [TIMEFRAME]?

3. [SYNTHESIS QUESTION]?
   a. What patterns emerge when comparing [SUB-QUESTION 3a]?
   b. What future developments might we anticipate?

Present your findings as a cohesive narrative that leads the reader through this logical progression.

Please prioritize [AUTHORITATIVE SOURCE TYPES] for your research, especially when addressing questions that require quantitative evidence or technical expertise. Avoid relying on [PROBLEMATIC SOURCE TYPES] which may contain unsupported claims or promotional content.
</code></pre></div></div>

<h3 id="3-the-diverse-perspectives-prompt">3. The Diverse Perspectives Prompt</h3>

<p>Ensure balanced research by explicitly requesting multiple viewpoints.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Research [TOPIC] from multiple perspectives, including:

1. The view from [PERSPECTIVE 1] (e.g., industry advocates)
2. The argument from [PERSPECTIVE 2] (e.g., critics or skeptics)
3. [PERSPECTIVE 3]'s approach to the issue (e.g., regulatory bodies)
4. How [PERSPECTIVE 4] is affected (e.g., consumers)

For each perspective, provide the strongest evidence and reasoning, credible sources, and note any limitations or biases. Conclude with a balanced synthesis that weighs these different viewpoints.

When sourcing each perspective, prioritize primary documents and official statements from relevant organizations rather than third-party interpretations. For quantitative claims, favor peer-reviewed research and official statistics over anecdotal evidence or promotional materials.
</code></pre></div></div>

<h3 id="4-the-format-detective">4. The Format Detective</h3>

<p>Specify exactly how you want information presented for maximum usefulness.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Research [TOPIC] and structure your findings in this specific format:

EXECUTIVE SUMMARY (200 words max)
- Key insight 1
- Key insight 2
- Key insight 3

SECTION 1: [ASPECT 1]
- Include [SPECIFIC DATA TYPE] in table format
- Highlight regional variations

SECTION 2: [ASPECT 2]
- Present as a chronological development
- Include major milestones

SECTION 3: [ASPECT 3]
- Focus on comparative analysis
- Include expert quotations when available

IMPLICATIONS &amp; RECOMMENDATIONS
- 3-5 actionable takeaways
- Potential scenarios to monitor

SOURCES
- Prioritize [SOURCE TYPE 1] and [SOURCE TYPE 2]
- Include assessment of source credibility
- Clearly distinguish between primary sources (e.g., original research, official documents) and secondary analyses
</code></pre></div></div>

<h2 id="the-fine-art-of-error-detection">The Fine Art of Error Detection</h2>

<p>Even with its impressive capabilities, Deep Research isn’t infallible. Here are the common error types to watch for:</p>

<ol>
  <li><strong>Citation Slips</strong>: Sometimes it might slightly misinterpret a source or get a specific number wrong. Always check critical statistics against their sources.</li>
  <li><strong>Overconfidence</strong>: The model may present speculative information with too much certainty. Be especially cautious with forecasts or emerging trends.</li>
  <li><strong>Source Quality</strong>: Deep Research will use whatever sources it finds. It might cite Wikipedia heavily or rely on less authoritative websites if better sources aren’t readily available.</li>
  <li><strong>Outdated Information</strong>: While it searches the live web, it might not always find the most recent data, especially in rapidly evolving fields.</li>
  <li><strong>Bias Inheritance</strong>: If the available sources on a topic lean in a particular direction, the report might reflect those biases unless specifically prompted to seek balance.</li>
</ol>

<p><strong>The Error-Proofing Prompt Addon:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>As you conduct this research, please:
- Indicate your confidence level for major claims
- Flag areas where reliable information is limited
- Note conflicting data or viewpoints when they exist
- Distinguish between established facts and emerging trends
- Identify any potential biases in the available sources
- Clearly indicate when a source might have commercial interests related to the topic
- Note when information comes from primary vs. secondary sources
</code></pre></div></div>

<h2 id="in-conclusion-your-deep-research-journey-awaits">In Conclusion: Your Deep Research Journey Awaits!</h2>

<p>With these prompting superpowers, you’re now equipped to extract maximum value from our 120 monthly Deep Research queries. Remember these key principles:</p>

<ol>
  <li><strong>Be specific and structured</strong> in your prompts</li>
  <li><strong>Guide source selection</strong> by specifying high-quality sources to prioritize</li>
  <li><strong>Embrace the clarification process</strong> to refine your query</li>
  <li><strong>Monitor but don’t micromanage</strong> the research process</li>
  <li><strong>Verify critical information</strong> in the resulting report</li>
  <li><strong>Iterate thoughtfully</strong> to build on initial research</li>
</ol>

<p>Most importantly, approach Deep Research as a collaborative partner rather than just a search tool. It’s designed to think, reason, and explore—often finding connections and insights you might not have considered.</p>

<p>Now go forth and research brilliantly! Your clients’ jaws are about to drop when they see the depth and quality of insights you’ll be delivering in record time. Just remember: with great AI comes great responsibility—use your 120 queries wisely!</p>

<p><strong>Final Thought</strong>: If you discover particularly effective prompting techniques or creative use cases, please share them with the team. We’re all learning how to harness this powerful new tool together!</p>

<hr />

<p><em>“The best research doesn’t just answer questions—it helps you ask better ones.”</em><br />
<em>— Anonymous Knowledge Services Analyst (who may or may not be an AI)</em></p>]]></content><author><name>Ben Reeve</name></author><category term="tutorials" /><summary type="html"><![CDATA[Welcome to the Knowledge Navigators Club! Hello brilliant minds of our Knowledge Services team! You’ve just been granted access to what might be the most powerful research assistant since the invention of the internet itself: OpenAI’s Deep Research. This isn’t your garden-variety AI—it’s a research powerhouse that can dive into the depths of the web, synthesize information from hundreds of sources, and deliver comprehensive, analyst-grade reports that would normally take us mere mortals hours (or days) to compile. But with great power comes great responsibility—and a monthly limit of 120 queries. So let’s make every Deep Research adventure count! ⚠️ CRITICAL REMINDER: Deep Research connects to the live internet. Never, ever input confidential client information, internal data, or sensitive material into your prompts. Keep your queries focused on publicly available information only. What happens in our Knowledge Management system stays in our Knowledge Management system! Why Deep Research Is Your New Best Friend Before we dive into the prompting wizardry, let’s understand what makes Deep Research special. It was trained using Reinforcement Learning on “hard browsing and reasoning tasks,” which is fancy AI-speak for “this thing knows how to research like a pro.” Its superpower is dynamic adaptation—it adjusts its research strategy as it goes, deciding what to search for next based on what it just learned. Think of it as the difference between following a rigid research plan and having a curious, intelligent assistant who can pivot when they discover something interesting or unexpected. While it’s pretty good at figuring things out on its own, upfront guidance in your prompt helps tremendously at those critical decision points. Consider your prompt as setting the GPS coordinates before the AI embarks on its research journey—the clearer your directions, the more targeted the destination. The Art of the Perfect Prompt 1. Structure is Your Friend Deep Research responds beautifully to structured prompts that clearly define what you want. Think of your prompt as a research brief you’d give to a junior analyst. Template: The Basic Research Brief I need comprehensive research on [TOPIC]. Please focus on: - [ASPECT 1] from [TIMEFRAME] - [ASPECT 2] with emphasis on [SPECIFIC ANGLE] - [ASPECT 3] including [SPECIFIC DATA TYPE] Prioritize authoritative sources such as academic journals, government publications, and industry reports. For financial data, favor company financial reports and regulatory filings over consulting firm marketing materials or opinion pieces. Structure the report with an executive summary, key findings, and detailed analysis. Include relevant statistics and cite all sources. Real Example: I need comprehensive research on autonomous vehicle regulations. Please focus on: - Current regulatory frameworks in the US, EU, and China from 2020-present - Safety standards with emphasis on liability issues - Market adoption rates including quarterly growth statistics Prioritize authoritative sources such as government regulatory documents, transportation department publications, and official industry standards. Avoid relying on consulting firm marketing materials or speculative blog posts. Structure the report with an executive summary, key findings, and detailed analysis. Include relevant statistics and cite all sources. 2. The Secret Sauce: Clarity, Detail, and Context Ambiguity is the enemy of good research. The more specific you are about scope, focus, and format, the better your results will be. Deep Research will ask clarifying questions if your prompt is too vague (which is actually helpful!), but you’ll save time by being detailed upfront. Template: The Detail-Rich Brief Analyze [SPECIFIC TOPIC] within [SPECIFIC PARAMETERS]. I'm specifically interested in: 1. [KEY QUESTION 1]? 2. [KEY QUESTION 2]? 3. [KEY QUESTION 3]? For context, this research will be used for [PURPOSE], so please emphasize [RELEVANT ASPECTS] in your analysis. Include [SPECIFIC DATA TYPE] if available, and present the information in [PREFERRED FORMAT]. When researching, please prioritize [PREFERRED SOURCE TYPES] such as [EXAMPLES], and minimize reliance on [SOURCES TO AVOID]. Real Example: Analyze the impact of ESG investing on financial returns within public equity markets from 2018-2023. I'm specifically interested in: 1. How do ESG-focused funds compare to traditional funds in terms of risk-adjusted returns? 2. What sectors show the strongest correlation between ESG ratings and financial performance? 3. How have investor attitudes toward ESG changed since the pandemic? For context, this research will be used for a client presentation to financial advisors, so please emphasize practical implications in your analysis. Include quantitative performance data if available, and present the information in a structured format with clear sections for each question. When researching, please prioritize SEC filings, fund prospectuses, academic finance journals, and financial market data from established providers such as Bloomberg or Morningstar, and minimize reliance on asset management marketing materials or opinion pieces without supporting data. 3. The Clarification Dance When you submit your prompt, Deep Research will often respond with a brief clarification conversation before diving in. This isn’t a bug—it’s a feature! Treat this as an opportunity to refine your research parameters. Tips for Effective Clarifications: Answer clarifying questions thoroughly Use this opportunity to add any details you forgot in your initial prompt Be decisive when presented with options (global vs. regional focus, etc.) Confirm or correct the AI’s understanding of your query Reinforce source quality requirements if needed Pro Tip: Sometimes you can anticipate common clarification questions and address them proactively in your initial prompt. For example: “Please take a global perspective unless specified otherwise” or “Focus on peer-reviewed studies rather than news articles when possible.” 4. Unleashing the Reasoning Beast Deep Research uses what they call an “o3 reasoning model,” which basically means it thinks deeply before answering. Once you’ve set up a good prompt and addressed clarifications, let it cook! The model may take 5-30 minutes to compile a report, during which it’s searching, reading, and synthesizing information. Don’t interrupt it—patience yields better results. The model is doing internal chain-of-thought reasoning that leads to deeper analysis. Template: The Reasoning Enhancer For this research on [TOPIC], I'd like you to: 1. First outline your approach to answering this question 2. Gather information from diverse sources, including [SUGGESTED SOURCE TYPES] 3. Consider multiple perspectives, especially [PERSPECTIVE 1] and [PERSPECTIVE 2] 4. Synthesize the findings into a cohesive analysis 5. Provide recommendations based on the evidence Prioritize primary sources including [SPECIFIC SOURCE TYPES] and be skeptical of [SOURCES TO TREAT WITH CAUTION]. When evaluating sources, favor those with robust methodologies and transparent data over opinion-based content. The Technology Trend Brief Research the current state and future trajectory of [TECHNOLOGY] with focus on: 1. Stage of development and technological maturity 2. Key innovations and breakthroughs in the past [TIMEFRAME] 3. Major companies and research institutions advancing the technology 4. Practical applications and use cases, both current and potential 5. Regulatory considerations and ethical implications 6. Market adoption forecasts and barriers Include a timeline of significant developments and cite reputable technical sources. Please prioritize these source types: - Academic journals and conference proceedings - Patent filings and technical documentation - Research institution publications - Company technical whitepapers (not marketing materials) - Regulatory and standards body publications Please minimize reliance on: - General media articles without technical depth - Vendor marketing materials without technical substantiation - Predictions without methodology explanations The Regulatory Landscape Brief Analyze the regulatory environment for [INDUSTRY/ACTIVITY] across [GEOGRAPHIC SCOPE], including: 1. Current regulatory frameworks and governing bodies 2. Recent significant regulatory changes 3. Pending legislation or regulatory proposals 4. Compliance requirements and challenges 5. International variations and harmonization efforts 6. Expert opinions on regulatory trends and future direction For this analysis, please strongly prioritize: - Official government and regulatory body publications - Legislative texts and court decisions - Regulatory agency guidance documents and enforcement actions - Bar association or legal journal analyses by practicing attorneys - Official international organization publications (e.g., OECD, WTO) Please avoid or minimize reliance on: - Non-specialist news summaries - Company marketing materials about regulatory compliance - Opinion pieces without legal expertise - Outdated regulatory information Deep Research Hacks: Insider Tips and Tricks 1. The Meta-Prompt Strategy Use regular GPT-4 to help craft your Deep Research prompt before using one of your precious 120 monthly queries. This “prompt about a prompt” approach helps refine your query for maximum effectiveness. I want to use OpenAI's Deep Research to gather information about [TOPIC]. Help me craft an optimal prompt that will: 1. Clearly define the scope and parameters 2. Specify the most relevant aspects to research 3. Include any necessary context 4. Structure the request for maximum clarity 5. Specify high-quality sources to prioritize and low-quality sources to avoid The resulting prompt should be comprehensive yet focused, as I want to make the most of this research query. 2. The Socratic Approach For complex topics, consider framing your prompt as a series of nested questions that build upon each other. Research [MAIN TOPIC] by exploring these interconnected questions: 1. [FOUNDATIONAL QUESTION]? a. How does this relate to [SUB-QUESTION 1a]? b. What are the implications for [SUB-QUESTION 1b]? 2. [BUILDING QUESTION]? a. What evidence supports or contradicts [SUB-QUESTION 2a]? b. How has this changed over [TIMEFRAME]? 3. [SYNTHESIS QUESTION]? a. What patterns emerge when comparing [SUB-QUESTION 3a]? b. What future developments might we anticipate? Present your findings as a cohesive narrative that leads the reader through this logical progression. Please prioritize [AUTHORITATIVE SOURCE TYPES] for your research, especially when addressing questions that require quantitative evidence or technical expertise. Avoid relying on [PROBLEMATIC SOURCE TYPES] which may contain unsupported claims or promotional content. 3. The Diverse Perspectives Prompt Ensure balanced research by explicitly requesting multiple viewpoints. Research [TOPIC] from multiple perspectives, including: 1. The view from [PERSPECTIVE 1] (e.g., industry advocates) 2. The argument from [PERSPECTIVE 2] (e.g., critics or skeptics) 3. [PERSPECTIVE 3]'s approach to the issue (e.g., regulatory bodies) 4. How [PERSPECTIVE 4] is affected (e.g., consumers) For each perspective, provide the strongest evidence and reasoning, credible sources, and note any limitations or biases. Conclude with a balanced synthesis that weighs these different viewpoints. When sourcing each perspective, prioritize primary documents and official statements from relevant organizations rather than third-party interpretations. For quantitative claims, favor peer-reviewed research and official statistics over anecdotal evidence or promotional materials. 4. The Format Detective Specify exactly how you want information presented for maximum usefulness. Research [TOPIC] and structure your findings in this specific format: EXECUTIVE SUMMARY (200 words max) - Key insight 1 - Key insight 2 - Key insight 3 SECTION 1: [ASPECT 1] - Include [SPECIFIC DATA TYPE] in table format - Highlight regional variations SECTION 2: [ASPECT 2] - Present as a chronological development - Include major milestones SECTION 3: [ASPECT 3] - Focus on comparative analysis - Include expert quotations when available IMPLICATIONS &amp; RECOMMENDATIONS - 3-5 actionable takeaways - Potential scenarios to monitor SOURCES - Prioritize [SOURCE TYPE 1] and [SOURCE TYPE 2] - Include assessment of source credibility - Clearly distinguish between primary sources (e.g., original research, official documents) and secondary analyses The Fine Art of Error Detection Even with its impressive capabilities, Deep Research isn’t infallible. Here are the common error types to watch for: Citation Slips: Sometimes it might slightly misinterpret a source or get a specific number wrong. Always check critical statistics against their sources. Overconfidence: The model may present speculative information with too much certainty. Be especially cautious with forecasts or emerging trends. Source Quality: Deep Research will use whatever sources it finds. It might cite Wikipedia heavily or rely on less authoritative websites if better sources aren’t readily available. Outdated Information: While it searches the live web, it might not always find the most recent data, especially in rapidly evolving fields. Bias Inheritance: If the available sources on a topic lean in a particular direction, the report might reflect those biases unless specifically prompted to seek balance. The Error-Proofing Prompt Addon: As you conduct this research, please: - Indicate your confidence level for major claims - Flag areas where reliable information is limited - Note conflicting data or viewpoints when they exist - Distinguish between established facts and emerging trends - Identify any potential biases in the available sources - Clearly indicate when a source might have commercial interests related to the topic - Note when information comes from primary vs. secondary sources In Conclusion: Your Deep Research Journey Awaits! With these prompting superpowers, you’re now equipped to extract maximum value from our 120 monthly Deep Research queries. Remember these key principles: Be specific and structured in your prompts Guide source selection by specifying high-quality sources to prioritize Embrace the clarification process to refine your query Monitor but don’t micromanage the research process Verify critical information in the resulting report Iterate thoughtfully to build on initial research Most importantly, approach Deep Research as a collaborative partner rather than just a search tool. It’s designed to think, reason, and explore—often finding connections and insights you might not have considered. Now go forth and research brilliantly! Your clients’ jaws are about to drop when they see the depth and quality of insights you’ll be delivering in record time. Just remember: with great AI comes great responsibility—use your 120 queries wisely! Final Thought: If you discover particularly effective prompting techniques or creative use cases, please share them with the team. We’re all learning how to harness this powerful new tool together! “The best research doesn’t just answer questions—it helps you ask better ones.” — Anonymous Knowledge Services Analyst (who may or may not be an AI)]]></summary></entry><entry><title type="html">DeepCredit v0.2: Advancing AI-Powered Credit Analysis</title><link href="http://localhost:4000/experiments/2025/04/06/deepcredit-v02-advancing-ai-credit-analysis.html" rel="alternate" type="text/html" title="DeepCredit v0.2: Advancing AI-Powered Credit Analysis" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/experiments/2025/04/06/deepcredit-v02-advancing-ai-credit-analysis</id><content type="html" xml:base="http://localhost:4000/experiments/2025/04/06/deepcredit-v02-advancing-ai-credit-analysis.html"><![CDATA[<p><img src="/assets/images/posts/1BCDD76C-DC19-46A3-9E0A-3C2C62B5C08C.png" alt="DeepCredit v0.2" class="align-center" style="max-width: 100%;" /></p>

<h2 id="executive-summary">Executive Summary</h2>

<p>This report documents the progress of DeepCredit, our custom AI system for generating comprehensive credit rating reports. The experiment compares our improved DeepCredit v0.2 against OpenAI’s Deep Research, with both systems tasked with producing credit rating reports for Trafigura Group Pte. Ltd.</p>

<p>Results show that DeepCredit v0.2 achieved significant improvement over v0.1, narrowing the performance gap with OpenAI’s system. While Deep Research still maintains an advantage, DeepCredit v0.2 demonstrated marked progress in research depth, analytical quality, and rating justification. Key findings include:</p>

<ul>
  <li>DeepCredit v0.2 earned an overall score of ~87/100 compared to Deep Research’s ~93/100</li>
  <li>The rating gap between systems narrowed, with DeepCredit v0.2 assigning an A (Stable) rating versus Deep Research’s BBB+ (Stable)</li>
  <li>Notable improvements in information quality, report structure, and rating justification</li>
</ul>

<p>The system’s enhanced architecture, featuring improved knowledge gap identification and prioritization, better tool selection, and comprehensive logging, contributed to these gains. This experiment confirms that our agentic approach shows promise but suggests a need to reimagine the system architecture to further close the performance gap.</p>

<h2 id="background">Background</h2>

<p>In our initial experiment, DeepCredit v0.1 substantially underperformed OpenAI’s Deep Research, scoring approximately 70-75/100 compared to Deep Research’s 93/100. The first version employed a multi-agent architecture based on our “Agentic Deep Research” framework with specialized agents, but had limited capabilities restricted to web scraping with no PDF parsing or advanced computational functions.</p>

<p>This shortfall prompted a targeted redesign of DeepCredit to enhance its research capabilities, knowledge management, and decision-making processes. The goal was to narrow the performance gap with OpenAI’s system while gaining deeper insights into which architectural components most significantly impact performance in credit analysis tasks.</p>

<h2 id="system-architecture-enhancements">System Architecture Enhancements</h2>

<p>DeepCredit v0.2 features significant architectural improvements across several dimensions, maintaining the multi-agent framework while enhancing each component’s capabilities:</p>

<h3 id="1-enhanced-knowledge-gap-agent">1. Enhanced Knowledge Gap Agent</h3>

<p>The Knowledge Gap Agent represents the system’s ability to identify what information is missing and prioritize research efforts accordingly. Key improvements include:</p>

<ul>
  <li><strong>Improved gap identification and prioritization</strong>: More sophisticated algorithms for identifying critical information gaps in financial data, corporate governance, market positioning, and risk factors</li>
  <li><strong>Gap history tracking</strong>: Implementation of persistent memory to track previously attempted research directions, avoiding repetitive or unproductive paths</li>
  <li><strong>Research completeness evaluation</strong>: Development of a weighted scoring system that quantifies the completeness of information across key credit rating dimensions</li>
  <li><strong>Outstanding gap identification</strong>: Enhanced capability to flag areas where information remains insufficient despite research attempts</li>
</ul>

<h3 id="2-upgraded-tool-selector-agent">2. Upgraded Tool Selector Agent</h3>

<p>The Tool Selector Agent determines which research tools to employ based on identified knowledge gaps. Enhancements focused on:</p>

<ul>
  <li><strong>Integration with Knowledge Gap Agent</strong>: Tighter coordination between gap identification and tool selection through shared context and evaluation criteria</li>
  <li><strong>Improved tool selection logic</strong>: More nuanced decision-making about which research approach would be most effective for specific types of information gaps</li>
  <li><strong>Previous attempt awareness</strong>: Capacity to consider the success or failure of previous tool selections when making new decisions</li>
  <li><strong>Research strategy optimization</strong>: Better matching of research tools to information needs based on the nature and priority of knowledge gaps</li>
</ul>

<h3 id="3-iterative-research-process-improvements">3. Iterative Research Process Improvements</h3>

<p>The core research workflow was redesigned to support more systematic and progressive information gathering:</p>

<ul>
  <li><strong>Evaluation result tracking</strong>: Implementation of a mechanism to store and utilize previous evaluation results when planning subsequent research</li>
  <li><strong>Priority-based research allocation</strong>: Modified run method that allocates research effort proportionally to the criticality of different knowledge gaps</li>
  <li><strong>Gap evaluation enhancement</strong>: Updated evaluation methods incorporating priority assessment and information quality metrics</li>
  <li><strong>Agent selection refinement</strong>: More sophisticated agent selection considering both gap history and relative priorities</li>
  <li><strong>Initialization improvements</strong>: Enhanced setup processes for evaluation tracking in the initialization method</li>
</ul>

<h3 id="4-new-logging-system">4. New Logging System</h3>

<p>A comprehensive logging infrastructure was implemented to track system behavior and performance:</p>

<ul>
  <li><strong>DeepCreditLogger class</strong>: Creation of a dedicated logging class with specialized functionality for tracking agentic research</li>
  <li><strong>Multi-level logging</strong>: Implementation of differentiated log levels (HIGH_LEVEL, PROGRESS, DETAIL, ERROR, WARNING) for varied granularity</li>
  <li><strong>Standardized naming convention</strong>: Establishment of consistent log file naming: YYYYMMDD_HHMMSS_MODE_COMPANYNAME.log</li>
  <li><strong>Comprehensive event tracking</strong>: Detailed logging of API calls, knowledge gaps, tool selections, research results, decision processes, and performance metrics</li>
</ul>

<h3 id="5-project-structure-improvements">5. Project Structure Improvements</h3>

<p>The system’s overall architecture was enhanced for better organization and maintainability:</p>

<ul>
  <li><strong>Dedicated logs directory</strong>: Addition of structured storage for log files</li>
  <li><strong>Updated initialization</strong>: Modified <strong>init</strong>.py to expose the new DeepCreditLogger class</li>
  <li><strong>Separation of concerns</strong>: Maintenance of clear boundaries between the main entry point and utility functions</li>
</ul>

<h2 id="experimental-results">Experimental Results</h2>

<p>The comparison between DeepCredit v0.2 and OpenAI’s Deep Research was conducted using the same evaluation criteria as our previous experiment, with both systems producing credit rating reports for Trafigura Group Pte. Ltd.</p>

<h3 id="rating-outcomes">Rating Outcomes</h3>

<p>A notable difference emerged in the final ratings:</p>

<ul>
  <li><strong>DeepCredit v0.2</strong>: A (Stable)</li>
  <li><strong>OpenAI Deep Research</strong>: BBB+ (Stable)</li>
</ul>

<p>This one-notch differential highlights how the systems interpreted similar information differently, with DeepCredit v0.2 taking a somewhat more optimistic view of Trafigura’s credit profile.</p>

<h3 id="performance-scores">Performance Scores</h3>

<p>An independent evaluation assessed both reports across three dimensions:</p>

<h4 id="1-quality-and-detail-of-information">1. Quality and Detail of Information</h4>

<ul>
  <li><strong>DeepCredit v0.2</strong>: 85/100</li>
  <li><strong>OpenAI Deep Research</strong>: 95/100</li>
</ul>

<p>DeepCredit v0.2 showed substantial improvement in information gathering compared to v0.1 (which scored 25/35 in the previous evaluation framework). The system effectively captured key financial metrics, discussed Trafigura’s business lines, and referenced liquidity positions. However, it still lacked some of the granularity found in the Deep Research report, particularly regarding specific risk incidents and detailed debt structure breakdowns.</p>

<h4 id="2-overall-quality-of-the-report">2. Overall Quality of the Report</h4>

<ul>
  <li><strong>DeepCredit v0.2</strong>: 90/100</li>
  <li><strong>OpenAI Deep Research</strong>: 90/100</li>
</ul>

<p>In this category, DeepCredit v0.2 achieved parity with Deep Research, presenting a well-structured report with clear section organization. The report featured a professional format with headings for methodology, business profile, industry factors, peer comparisons, and rating sensitivities. This represents significant progress from v0.1, which produced more of a skeletal outline than a comprehensive analysis.</p>

<h4 id="3-justification-of-final-rating">3. Justification of Final Rating</h4>

<ul>
  <li><strong>DeepCredit v0.2</strong>: 85/100</li>
  <li><strong>OpenAI Deep Research</strong>: 95/100</li>
</ul>

<p>While DeepCredit v0.2 provided clearer rating justification than v0.1, explaining the A rating based on strong liquidity, large credit facilities, diversification, and renewable energy initiatives, it still didn’t match the depth of Deep Research’s justification. The Deep Research report offered more detailed explanations of how specific financial metrics and operational risks translated into the BBB+ rating.</p>

<h4 id="overall-scores">Overall Scores</h4>

<ul>
  <li><strong>DeepCredit v0.2</strong>: ~87/100</li>
  <li><strong>OpenAI Deep Research</strong>: ~93/100</li>
</ul>

<p>DeepCredit v0.2 narrowed the performance gap significantly, improving from approximately 70-75/100 to ~87/100, while Deep Research maintained its high performance at ~93/100.</p>

<h2 id="key-differences-in-output-quality">Key Differences in Output Quality</h2>

<h3 id="strengths-of-deepcredit-v02">Strengths of DeepCredit v0.2</h3>

<ol>
  <li><strong>Improved Structure</strong>: The report featured well-organized sections following a conventional rating agency format, enhancing readability and professional appearance.</li>
  <li><strong>Balance of Information</strong>: The system effectively balanced quantitative data with qualitative analysis, providing context for financial metrics and industry trends.</li>
  <li><strong>Forward-Looking Analysis</strong>: DeepCredit v0.2 incorporated more forward-looking elements, including Trafigura’s strategic initiatives in renewables and green hydrogen.</li>
  <li><strong>ESG Integration</strong>: The report placed stronger emphasis on environmental sustainability factors, potentially influencing its more optimistic rating outcome.</li>
  <li><strong>Peer Comparison</strong>: Enhanced competitive analysis positioned Trafigura relative to peers like Glencore, Vitol, and Mercuria.</li>
</ol>

<h3 id="remaining-gaps">Remaining Gaps</h3>

<ol>
  <li><strong>Detail Depth</strong>: Deep Research still provided more granular financial metrics, risk incident details, and specific numeric trends.</li>
  <li><strong>Risk Assessment</strong>: Deep Research’s more comprehensive evaluation of fraud incidents, governance lapses, and event risks likely contributed to its more conservative rating.</li>
  <li><strong>Historical Context</strong>: The OpenAI system incorporated more detailed historical references and year-by-year numeric changes.</li>
  <li><strong>Rating Framework</strong>: Deep Research more explicitly linked its analysis to established rating agency methodologies and explained the rationale for not assigning an A rating.</li>
  <li><strong>External References</strong>: Deep Research included more citations to third-party sources and rating agency perspectives.</li>
</ol>

<h2 id="analysis-of-architectural-impact">Analysis of Architectural Impact</h2>

<p>The improvements in DeepCredit v0.2’s performance can be directly linked to specific architectural enhancements:</p>

<ol>
  <li><strong>Enhanced Knowledge Gap Agent</strong>: The improved gap identification and prioritization resulted in more comprehensive coverage of essential credit rating dimensions. The system still missed some specific details but showed much better awareness of what information was needed.</li>
  <li><strong>Upgraded Tool Selector Agent</strong>: Better tool selection led to more efficient research, allowing the system to gather relevant information about Trafigura’s business operations, financial performance, and industry positioning.</li>
  <li><strong>Iterative Research Process</strong>: The refined research workflow enabled progressive information gathering, evidenced by the report’s balanced coverage across multiple dimensions of creditworthiness.</li>
  <li><strong>Logging System</strong>: While not directly affecting output quality, the comprehensive logging facilitated better understanding of the system’s decision-making processes and identified areas for further improvement.</li>
</ol>

<p>These enhancements collectively enabled DeepCredit v0.2 to produce a more thorough, balanced, and professional credit rating report. However, the remaining performance gap suggests that certain architectural limitations still constrain the system’s capabilities.</p>

<h2 id="implications-and-future-directions">Implications and Future Directions</h2>

<h3 id="current-limitations">Current Limitations</h3>

<p>Despite significant improvements, DeepCredit v0.2 faces several limitations:</p>

<ol>
  <li><strong>Architectural Complexity</strong>: The multi-agent system has become increasingly complex, making it difficult to isolate which components contribute most to performance gains.</li>
  <li><strong>Component Interdependence</strong>: The tight coupling between agents creates dependencies that can propagate errors or inefficiencies throughout the system.</li>
  <li><strong>Optimization Challenges</strong>: The current architecture makes it difficult to optimize individual components without affecting others.</li>
  <li><strong>Evaluation Granularity</strong>: The current evaluation framework provides useful high-level feedback but lacks the granularity needed to guide specific architectural improvements.</li>
</ol>

<h3 id="planned-approach">Planned Approach</h3>

<p>To address these limitations and further advance DeepCredit’s capabilities, we plan to:</p>

<ol>
  <li><strong>Architectural Simplification</strong>: Strip the system back to its essential components to establish a cleaner baseline.</li>
  <li><strong>Robust Evaluation Framework</strong>: Develop more granular, component-specific evaluation metrics that can isolate the impact of individual architectural elements.</li>
  <li><strong>Incremental Reconstruction</strong>: Rebuild the system one component at a time, carefully measuring the performance impact of each addition.</li>
  <li><strong>Systematic Configuration Testing</strong>: Test multiple configurations of each component to develop deeper insights into optimal settings.</li>
  <li><strong>Agentic Workflow Analysis</strong>: Focus on understanding which aspects of the agentic workflow contribute most significantly to performance and why.</li>
</ol>

<p>This approach will enable more targeted improvements and help establish a clearer understanding of the architectural principles that drive performance in AI-powered credit analysis.</p>

<h2 id="conclusion">Conclusion</h2>

<p>DeepCredit v0.2 represents a significant advancement in our AI-powered credit analysis capabilities. The narrowing performance gap with OpenAI’s Deep Research demonstrates the potential of our agentic approach while highlighting areas for continued improvement.</p>

<p>The experiment confirms several key insights:</p>

<ol>
  <li>Multi-agent architectures can effectively approach complex analytical tasks like credit rating, but require careful design and coordination.</li>
  <li>Systematic knowledge gap identification and prioritization significantly impact research quality and comprehensiveness.</li>
  <li>Different architectural approaches can lead to divergent analytical conclusions, as evidenced by the one-notch rating difference.</li>
</ol>

<p>While DeepCredit v0.2 shows promise, its increasing complexity suggests that continued incremental improvements may yield diminishing returns. The planned redesign, focusing on systematic evaluation and component-by-component optimization, offers a more promising path toward closing the remaining performance gap with state-of-the-art systems.</p>

<p>By developing deeper insights into which aspects of the agentic workflow contribute most to performance, we aim to establish a more efficient and effective architecture for AI-powered credit analysis that could potentially exceed the capabilities of current leading systems.</p>]]></content><author><name>Ben Reeve</name></author><category term="experiments" /><summary type="html"><![CDATA[Executive Summary This report documents the progress of DeepCredit, our custom AI system for generating comprehensive credit rating reports. The experiment compares our improved DeepCredit v0.2 against OpenAI’s Deep Research, with both systems tasked with producing credit rating reports for Trafigura Group Pte. Ltd. Results show that DeepCredit v0.2 achieved significant improvement over v0.1, narrowing the performance gap with OpenAI’s system. While Deep Research still maintains an advantage, DeepCredit v0.2 demonstrated marked progress in research depth, analytical quality, and rating justification. Key findings include: DeepCredit v0.2 earned an overall score of ~87/100 compared to Deep Research’s ~93/100 The rating gap between systems narrowed, with DeepCredit v0.2 assigning an A (Stable) rating versus Deep Research’s BBB+ (Stable) Notable improvements in information quality, report structure, and rating justification The system’s enhanced architecture, featuring improved knowledge gap identification and prioritization, better tool selection, and comprehensive logging, contributed to these gains. This experiment confirms that our agentic approach shows promise but suggests a need to reimagine the system architecture to further close the performance gap. Background In our initial experiment, DeepCredit v0.1 substantially underperformed OpenAI’s Deep Research, scoring approximately 70-75/100 compared to Deep Research’s 93/100. The first version employed a multi-agent architecture based on our “Agentic Deep Research” framework with specialized agents, but had limited capabilities restricted to web scraping with no PDF parsing or advanced computational functions. This shortfall prompted a targeted redesign of DeepCredit to enhance its research capabilities, knowledge management, and decision-making processes. The goal was to narrow the performance gap with OpenAI’s system while gaining deeper insights into which architectural components most significantly impact performance in credit analysis tasks. System Architecture Enhancements DeepCredit v0.2 features significant architectural improvements across several dimensions, maintaining the multi-agent framework while enhancing each component’s capabilities: 1. Enhanced Knowledge Gap Agent The Knowledge Gap Agent represents the system’s ability to identify what information is missing and prioritize research efforts accordingly. Key improvements include: Improved gap identification and prioritization: More sophisticated algorithms for identifying critical information gaps in financial data, corporate governance, market positioning, and risk factors Gap history tracking: Implementation of persistent memory to track previously attempted research directions, avoiding repetitive or unproductive paths Research completeness evaluation: Development of a weighted scoring system that quantifies the completeness of information across key credit rating dimensions Outstanding gap identification: Enhanced capability to flag areas where information remains insufficient despite research attempts 2. Upgraded Tool Selector Agent The Tool Selector Agent determines which research tools to employ based on identified knowledge gaps. Enhancements focused on: Integration with Knowledge Gap Agent: Tighter coordination between gap identification and tool selection through shared context and evaluation criteria Improved tool selection logic: More nuanced decision-making about which research approach would be most effective for specific types of information gaps Previous attempt awareness: Capacity to consider the success or failure of previous tool selections when making new decisions Research strategy optimization: Better matching of research tools to information needs based on the nature and priority of knowledge gaps 3. Iterative Research Process Improvements The core research workflow was redesigned to support more systematic and progressive information gathering: Evaluation result tracking: Implementation of a mechanism to store and utilize previous evaluation results when planning subsequent research Priority-based research allocation: Modified run method that allocates research effort proportionally to the criticality of different knowledge gaps Gap evaluation enhancement: Updated evaluation methods incorporating priority assessment and information quality metrics Agent selection refinement: More sophisticated agent selection considering both gap history and relative priorities Initialization improvements: Enhanced setup processes for evaluation tracking in the initialization method 4. New Logging System A comprehensive logging infrastructure was implemented to track system behavior and performance: DeepCreditLogger class: Creation of a dedicated logging class with specialized functionality for tracking agentic research Multi-level logging: Implementation of differentiated log levels (HIGH_LEVEL, PROGRESS, DETAIL, ERROR, WARNING) for varied granularity Standardized naming convention: Establishment of consistent log file naming: YYYYMMDD_HHMMSS_MODE_COMPANYNAME.log Comprehensive event tracking: Detailed logging of API calls, knowledge gaps, tool selections, research results, decision processes, and performance metrics 5. Project Structure Improvements The system’s overall architecture was enhanced for better organization and maintainability: Dedicated logs directory: Addition of structured storage for log files Updated initialization: Modified init.py to expose the new DeepCreditLogger class Separation of concerns: Maintenance of clear boundaries between the main entry point and utility functions Experimental Results The comparison between DeepCredit v0.2 and OpenAI’s Deep Research was conducted using the same evaluation criteria as our previous experiment, with both systems producing credit rating reports for Trafigura Group Pte. Ltd. Rating Outcomes A notable difference emerged in the final ratings: DeepCredit v0.2: A (Stable) OpenAI Deep Research: BBB+ (Stable) This one-notch differential highlights how the systems interpreted similar information differently, with DeepCredit v0.2 taking a somewhat more optimistic view of Trafigura’s credit profile. Performance Scores An independent evaluation assessed both reports across three dimensions: 1. Quality and Detail of Information DeepCredit v0.2: 85/100 OpenAI Deep Research: 95/100 DeepCredit v0.2 showed substantial improvement in information gathering compared to v0.1 (which scored 25/35 in the previous evaluation framework). The system effectively captured key financial metrics, discussed Trafigura’s business lines, and referenced liquidity positions. However, it still lacked some of the granularity found in the Deep Research report, particularly regarding specific risk incidents and detailed debt structure breakdowns. 2. Overall Quality of the Report DeepCredit v0.2: 90/100 OpenAI Deep Research: 90/100 In this category, DeepCredit v0.2 achieved parity with Deep Research, presenting a well-structured report with clear section organization. The report featured a professional format with headings for methodology, business profile, industry factors, peer comparisons, and rating sensitivities. This represents significant progress from v0.1, which produced more of a skeletal outline than a comprehensive analysis. 3. Justification of Final Rating DeepCredit v0.2: 85/100 OpenAI Deep Research: 95/100 While DeepCredit v0.2 provided clearer rating justification than v0.1, explaining the A rating based on strong liquidity, large credit facilities, diversification, and renewable energy initiatives, it still didn’t match the depth of Deep Research’s justification. The Deep Research report offered more detailed explanations of how specific financial metrics and operational risks translated into the BBB+ rating. Overall Scores DeepCredit v0.2: ~87/100 OpenAI Deep Research: ~93/100 DeepCredit v0.2 narrowed the performance gap significantly, improving from approximately 70-75/100 to ~87/100, while Deep Research maintained its high performance at ~93/100. Key Differences in Output Quality Strengths of DeepCredit v0.2 Improved Structure: The report featured well-organized sections following a conventional rating agency format, enhancing readability and professional appearance. Balance of Information: The system effectively balanced quantitative data with qualitative analysis, providing context for financial metrics and industry trends. Forward-Looking Analysis: DeepCredit v0.2 incorporated more forward-looking elements, including Trafigura’s strategic initiatives in renewables and green hydrogen. ESG Integration: The report placed stronger emphasis on environmental sustainability factors, potentially influencing its more optimistic rating outcome. Peer Comparison: Enhanced competitive analysis positioned Trafigura relative to peers like Glencore, Vitol, and Mercuria. Remaining Gaps Detail Depth: Deep Research still provided more granular financial metrics, risk incident details, and specific numeric trends. Risk Assessment: Deep Research’s more comprehensive evaluation of fraud incidents, governance lapses, and event risks likely contributed to its more conservative rating. Historical Context: The OpenAI system incorporated more detailed historical references and year-by-year numeric changes. Rating Framework: Deep Research more explicitly linked its analysis to established rating agency methodologies and explained the rationale for not assigning an A rating. External References: Deep Research included more citations to third-party sources and rating agency perspectives. Analysis of Architectural Impact The improvements in DeepCredit v0.2’s performance can be directly linked to specific architectural enhancements: Enhanced Knowledge Gap Agent: The improved gap identification and prioritization resulted in more comprehensive coverage of essential credit rating dimensions. The system still missed some specific details but showed much better awareness of what information was needed. Upgraded Tool Selector Agent: Better tool selection led to more efficient research, allowing the system to gather relevant information about Trafigura’s business operations, financial performance, and industry positioning. Iterative Research Process: The refined research workflow enabled progressive information gathering, evidenced by the report’s balanced coverage across multiple dimensions of creditworthiness. Logging System: While not directly affecting output quality, the comprehensive logging facilitated better understanding of the system’s decision-making processes and identified areas for further improvement. These enhancements collectively enabled DeepCredit v0.2 to produce a more thorough, balanced, and professional credit rating report. However, the remaining performance gap suggests that certain architectural limitations still constrain the system’s capabilities. Implications and Future Directions Current Limitations Despite significant improvements, DeepCredit v0.2 faces several limitations: Architectural Complexity: The multi-agent system has become increasingly complex, making it difficult to isolate which components contribute most to performance gains. Component Interdependence: The tight coupling between agents creates dependencies that can propagate errors or inefficiencies throughout the system. Optimization Challenges: The current architecture makes it difficult to optimize individual components without affecting others. Evaluation Granularity: The current evaluation framework provides useful high-level feedback but lacks the granularity needed to guide specific architectural improvements. Planned Approach To address these limitations and further advance DeepCredit’s capabilities, we plan to: Architectural Simplification: Strip the system back to its essential components to establish a cleaner baseline. Robust Evaluation Framework: Develop more granular, component-specific evaluation metrics that can isolate the impact of individual architectural elements. Incremental Reconstruction: Rebuild the system one component at a time, carefully measuring the performance impact of each addition. Systematic Configuration Testing: Test multiple configurations of each component to develop deeper insights into optimal settings. Agentic Workflow Analysis: Focus on understanding which aspects of the agentic workflow contribute most significantly to performance and why. This approach will enable more targeted improvements and help establish a clearer understanding of the architectural principles that drive performance in AI-powered credit analysis. Conclusion DeepCredit v0.2 represents a significant advancement in our AI-powered credit analysis capabilities. The narrowing performance gap with OpenAI’s Deep Research demonstrates the potential of our agentic approach while highlighting areas for continued improvement. The experiment confirms several key insights: Multi-agent architectures can effectively approach complex analytical tasks like credit rating, but require careful design and coordination. Systematic knowledge gap identification and prioritization significantly impact research quality and comprehensiveness. Different architectural approaches can lead to divergent analytical conclusions, as evidenced by the one-notch rating difference. While DeepCredit v0.2 shows promise, its increasing complexity suggests that continued incremental improvements may yield diminishing returns. The planned redesign, focusing on systematic evaluation and component-by-component optimization, offers a more promising path toward closing the remaining performance gap with state-of-the-art systems. By developing deeper insights into which aspects of the agentic workflow contribute most to performance, we aim to establish a more efficient and effective architecture for AI-powered credit analysis that could potentially exceed the capabilities of current leading systems.]]></summary></entry><entry><title type="html">Becoming a GitHub Copilot Ninja</title><link href="http://localhost:4000/tutorials/2025/04/06/github-copilot-ninja.html" rel="alternate" type="text/html" title="Becoming a GitHub Copilot Ninja" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/tutorials/2025/04/06/github-copilot-ninja</id><content type="html" xml:base="http://localhost:4000/tutorials/2025/04/06/github-copilot-ninja.html"><![CDATA[<p><img src="/assets/images/posts/D4B8766D-7FA9-4ADB-BA93-9A1D2962B462_1_105_c.jpeg" alt="GitHub Copilot Ninja" class="align-center" style="max-width: 100%;" /></p>

<h2 id="executive-summary">Executive Summary</h2>

<p>Welcome, Financial Analysts! This guide will transform you from GitHub Copilot users into GitHub Copilot ninjas. While Copilot is impressive out-of-the-box, its true potential is unlocked through strategic usage techniques that push its capabilities to the bleeding edge.</p>

<p>Copilot works by generating code suggestions based on the context it sees. By default, it only “sees” your current file and perhaps a few open tabs. With the techniques in this guide, you’ll learn to:</p>

<ul>
  <li>Feed Copilot the right context to generate better, more contextually aware code</li>
  <li>Create system-like prompts that guide Copilot to follow your project’s rules and patterns</li>
  <li>Leverage advanced techniques to simulate whole-project awareness</li>
  <li>Optimize Copilot for quantitative Python workflows</li>
  <li>Work around Copilot’s limitations with fast-evolving libraries and APIs</li>
</ul>

<p>This guide is designed for Python-focused analysts working in VS Code and Jupyter Notebooks. Whether you’re new to Copilot or already using it daily, these techniques will significantly boost your productivity and code quality.</p>

<hr />

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li>Setting Up Your Copilot Environment</li>
  <li>Feeding Context to Copilot</li>
  <li>Strategic Prompting: The Art of Guiding Copilot</li>
  <li>Simulating Whole-Project Awareness</li>
  <li>Optimizing for Python Quantitative Analysis</li>
  <li>Handling API Changes and Library Evolution</li>
  <li>Copilot Ninja Workflows</li>
  <li>Advanced Techniques: Beyond the Basics</li>
  <li>Practical Examples for Financial Analysis</li>
  <li>Troubleshooting and Best Practices</li>
</ol>

<hr />

<h2 id="1-setting-up-your-copilot-environment">1. Setting Up Your Copilot Environment</h2>

<h3 id="basic-setup">Basic Setup</h3>

<ol>
  <li><strong>Install the GitHub Copilot extension in VS Code</strong>:
    <ul>
      <li>Search for “GitHub Copilot” in the VS Code extensions marketplace</li>
      <li>Click Install</li>
      <li>Sign in with your GitHub account (contact your admin if you need access)</li>
    </ul>
  </li>
  <li><strong>Configure Copilot settings</strong>:
    <ul>
      <li>Set your preferred keyboard shortcuts (<code class="language-plaintext highlighter-rouge">Ctrl+Enter</code> to accept suggestions)</li>
      <li>Enable inline suggestions (<code class="language-plaintext highlighter-rouge">Editor &gt; Inlay Hints</code> settings)</li>
      <li>Consider enabling the latest Copilot model if available (GPT-4 based model performs better)</li>
    </ul>
  </li>
  <li><strong>Jupyter Notebook Integration</strong>:
    <ul>
      <li>VS Code has native support for .ipynb notebooks with Copilot</li>
      <li>Simply open your notebook files in VS Code - Copilot works automatically in code cells</li>
      <li>For those using JupyterLab, community extensions exist but VS Code provides the most seamless experience</li>
    </ul>
  </li>
</ol>

<h3 id="enabling-copilot-chat-if-available">Enabling Copilot Chat (If Available)</h3>

<p>Copilot Chat adds conversational capabilities that regular Copilot doesn’t have:</p>

<ol>
  <li>Install the GitHub Copilot Chat extension in VS Code</li>
  <li>Use the <code class="language-plaintext highlighter-rouge">/</code> commands in chat for specialized functions:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">/explain</code> to understand complex code</li>
      <li><code class="language-plaintext highlighter-rouge">/fix</code> to debug errors</li>
      <li><code class="language-plaintext highlighter-rouge">/tests</code> to generate unit tests</li>
      <li><code class="language-plaintext highlighter-rouge">/workspace</code> to ask questions about your entire project</li>
    </ul>
  </li>
</ol>

<h3 id="optimal-ide-configuration">Optimal IDE Configuration</h3>

<p>Set up your VS Code environment to maximize Copilot’s effectiveness:</p>

<ol>
  <li><strong>Increase your editor font size</strong>: This gives Copilot more room to show inline suggestions</li>
  <li><strong>Configure split views</strong>: Use split panes to keep relevant files visible simultaneously, as this improves Copilot’s context awareness</li>
  <li><strong>Enable autosave</strong>: Helps Copilot learn from your edits quicker</li>
  <li><strong>Use a consistent color theme</strong>: Dark themes often make Copilot’s ghost text more readable</li>
</ol>

<hr />

<h2 id="2-feeding-context-to-copilot">2. Feeding Context to Copilot</h2>

<p>Copilot’s biggest limitation is its limited context window. By default, it only “sees” the file you’re currently editing and perhaps a few open tabs. Here’s how to feed it the right context:</p>

<h3 id="open-relevant-files">Open Relevant Files</h3>

<p>Keep related files open in your editor when working on a task. Copilot will access content from multiple open tabs when making suggestions:</p>

<ol>
  <li><strong>Module definitions</strong>: Keep imported module files open</li>
  <li><strong>Configuration files</strong>: Open settings, config, or schema files</li>
  <li><strong>Test files</strong>: Having tests open helps Copilot understand expected behavior</li>
  <li><strong>Related code</strong>: Open files with similar functionality</li>
</ol>

<p>As confirmed by GitHub engineers: “if two files are open – one that defines a class, and another with unit tests for that class – by looking at both files, we have more context…resulting in higher-quality suggestions.”</p>

<h3 id="creating-project-guide-documents">Creating Project Guide Documents</h3>

<p>Create dedicated guide files that Copilot can reference:</p>

<ol>
  <li><strong>Create a <code class="language-plaintext highlighter-rouge">COPILOT_GUIDE.md</code> or <code class="language-plaintext highlighter-rouge">PROJECT_RULES.md</code></strong> containing:
    <ul>
      <li>Architecture overview</li>
      <li>Tech stack details</li>
      <li>Coding style guidelines</li>
      <li>Performance constraints</li>
      <li>Common patterns in your codebase</li>
    </ul>
  </li>
  <li><strong>Example of a guide file</strong>:</li>
</ol>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh"># Project Guide for Financial Analysis Codebase</span>

<span class="gu">## Architecture</span>
<span class="p">-</span> We use a modular approach with these directories:
<span class="p">  -</span> data/ - For data loading and preprocessing
<span class="p">  -</span> models/ - For financial models and algorithms
<span class="p">  -</span> viz/ - For visualization components
<span class="p">  -</span> utils/ - For shared utility functions

<span class="gu">## Tech Stack</span>
<span class="p">-</span> Python 3.10+
<span class="p">-</span> pandas 2.0+ for data manipulation
<span class="p">-</span> numpy for numerical operations
<span class="p">-</span> scikit-learn for ML components
<span class="p">-</span> matplotlib and seaborn for visualization

<span class="gu">## Style Guidelines</span>
<span class="p">-</span> We use PEP8 with 100 character line limit
<span class="p">-</span> Type hints are required for all function parameters and returns
<span class="p">-</span> Use descriptive variable names (no single-letter variables except in math formulas)
<span class="p">-</span> Document functions using Google-style docstrings
<span class="p">-</span> Prefer pandas vectorized operations over loops for performance

<span class="gu">## Performance Constraints</span>
<span class="p">-</span> Must handle datasets up to 10GB in memory
<span class="p">-</span> All financial calculations must use Decimal for precise money handling
<span class="p">-</span> Analysis functions should be optimizable for parallel processing
</code></pre></div></div>

<ol>
  <li><strong>Keep this guide open</strong> in a tab while coding to influence Copilot’s suggestions</li>
</ol>

<h3 id="in-line-comment-directives">In-line Comment Directives</h3>

<p>Add strategic comments directly in your code to guide Copilot:</p>

<ol>
  <li><strong>Rule comments at the file top</strong>:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Copilot Instruction: This module uses numpy arrays for all data operations
# Do not use Python lists for numerical calculations
# Use vectorized operations rather than explicit loops
# Use Decimal for all financial calculations requiring precision
</span></code></pre></div></div>

<ol>
  <li><strong>Function guidance comments</strong>:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implement a function that calculates the covariance matrix
# Must handle missing values gracefully
# Should return a pandas DataFrame with labeled indices
</span><span class="k">def</span> <span class="nf">calculate_covariance</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Copilot will now suggest implementation based on these constraints
</span></code></pre></div></div>

<ol>
  <li><strong>Architecture reminders</strong>:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This class follows the repository pattern
# All database operations should be async
# Must validate input and handle exceptions
</span><span class="k">class</span> <span class="nc">MarketDataRepository</span><span class="p">:</span>
</code></pre></div></div>

<hr />

<h2 id="3-strategic-prompting-the-art-of-guiding-copilot">3. Strategic Prompting: The Art of Guiding Copilot</h2>

<p>Prompting Copilot effectively is a skill that dramatically improves results:</p>

<h3 id="write-comments-before-implementation">Write Comments Before Implementation</h3>

<ol>
  <li><strong>Describe what you want before coding</strong>:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to normalize financial time series data
# Takes a DataFrame with 'date' and 'price' columns
# Performs the following:
# 1. Calculates daily returns
# 2. Handles missing values
# 3. Winsorizes outliers (beyond 3 standard deviations)
# 4. Scales values to zero mean and unit variance
# 5. Returns the normalized DataFrame with original date index
</span></code></pre></div></div>

<hr />

<h2 id="8-advanced-techniques-beyond-the-basics">8. Advanced Techniques: Beyond the Basics</h2>

<p>These techniques push Copilot to its limits:</p>

<h3 id="use-repetitive-hooks">Use Repetitive Hooks</h3>

<ol>
  <li><strong>Repeat important constraints</strong> in multiple places:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PROJECT RULE: All financial calculations must use Decimal for precision
</span>
<span class="k">def</span> <span class="nf">calculate_compound_returns</span><span class="p">(</span><span class="n">prices</span><span class="p">):</span>
    <span class="c1"># Must use Decimal for precision
</span>    
    <span class="c1"># Implementation...
</span>    
    <span class="c1"># Ensure Decimal is used for all calculations before returning
</span></code></pre></div></div>

<h3 id="multiple-suggestion-reviews">Multiple Suggestion Reviews</h3>

<ol>
  <li><strong>Cycle through alternative completions</strong> before accepting:
    <ul>
      <li>When Copilot suggests code, use <code class="language-plaintext highlighter-rouge">Alt+]</code> and <code class="language-plaintext highlighter-rouge">Alt+[</code> to cycle through alternatives</li>
      <li>Or hover over the Copilot icon to see alternative suggestions</li>
      <li>Choose the suggestion that best matches your needs</li>
    </ul>
  </li>
</ol>

<h3 id="maintain-a-prompt-template-file">Maintain a “Prompt Template” File</h3>

<ol>
  <li><strong>Create a PROMPT_TEMPLATES.md</strong> file with effective prompts for common tasks:</li>
</ol>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh"># Copilot Prompt Templates</span>

<span class="gu">## For Data Loading Functions:</span>

<span class="gh"># Function to load financial data from {format} file</span>
<span class="gh"># Must handle errors gracefully with appropriate messages</span>
<span class="gh"># Should return a pandas DataFrame with columns: {columns}</span>
<span class="gh"># Performance requirement: Must handle files up to {size} GB</span>

<span class="gu">## For Visualization Functions:</span>

<span class="gh"># Create a {plot_type} visualization showing {what_to_show}</span>
<span class="gh"># X-axis represents {x_variable}</span>
<span class="gh"># Y-axis represents {y_variable}</span>
<span class="gh"># Use {color_scheme} color palette</span>
<span class="gh"># Add proper labels, title, and legend</span>
<span class="gh"># Make sure visualization is accessible (colorblind-friendly)</span>
</code></pre></div></div>

<ol>
  <li><strong>Copy and customize</strong> these templates when needed</li>
</ol>

<h3 id="pr-driven-development">PR-Driven Development</h3>

<ol>
  <li><strong>Write detailed PR descriptions</strong> that explain changes across multiple files</li>
  <li><strong>Reference these descriptions</strong> in your code comments</li>
  <li><strong>Use the PR as a prompt</strong> for Copilot to understand the big picture</li>
</ol>

<h3 id="custom-instructions-for-copilot-chat">Custom Instructions for Copilot Chat</h3>

<p>If your organization has enabled custom instructions for Copilot Chat:</p>

<ol>
  <li>Create a <code class="language-plaintext highlighter-rouge">.github/copilot/instructions.md</code> file with project-specific guidance</li>
  <li>Include coding standards, architecture constraints, and common patterns</li>
  <li>These instructions will be injected into Copilot Chat’s context automatically</li>
</ol>

<hr />

<h2 id="9-practical-examples-for-financial-analysis">9. Practical Examples for Financial Analysis</h2>

<p>Let’s apply these techniques to common financial analysis tasks:</p>

<h3 id="example-1-market-data-processing-pipeline">Example 1: Market Data Processing Pipeline</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Goal: Create a pipeline to process raw market data
# Follow these project guidelines:
# - Use pandas for data handling
# - Implement efficient data cleaning
# - Handle missing values, outliers, and data adjustments
# - Calculate key technical indicators
# - Store processed data in standardized format
</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">decimal</span> <span class="kn">import</span> <span class="n">Decimal</span>

<span class="k">def</span> <span class="nf">process_market_data</span><span class="p">(</span>
    <span class="n">ticker_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">start_date</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">end_date</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">indicators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s">"SMA"</span><span class="p">,</span> <span class="s">"EMA"</span><span class="p">,</span> <span class="s">"RSI"</span><span class="p">,</span> <span class="s">"MACD"</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="s">"""
    Process market data for multiple tickers and calculate technical indicators.
    
    Args:
        ticker_list: List of ticker symbols
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
        indicators: List of technical indicators to calculate
        
    Returns:
        DataFrame with processed market data and indicators
    """</span>
    <span class="c1"># Let Copilot implement the function
</span></code></pre></div></div>

<h3 id="example-2-portfolio-analysis-with-visualization">Example 2: Portfolio Analysis with Visualization</h3>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gu">## Analyzing Portfolio Performance Against Benchmarks</span>

We need to:
<span class="p">1.</span> Load portfolio allocation and return data
<span class="p">2.</span> Calculate key performance metrics (returns, volatility, Sharpe, drawdown)
<span class="p">3.</span> Compare against market benchmarks (S&amp;P 500, sector indices)
<span class="p">4.</span> Perform attribution analysis to identify sources of alpha
<span class="p">5.</span> Visualize the results using appropriate plots
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implementation following the analysis plan above
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>

<span class="c1"># Load portfolio and benchmark data
</span><span class="n">portfolio_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'portfolio_returns.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'date'</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">benchmark_returns</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'benchmark_returns.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'date'</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Let Copilot continue implementing based on the markdown description
</span></code></pre></div></div>

<h3 id="example-3-machine-learning-for-return-prediction">Example 3: Machine Learning for Return Prediction</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implement a model to predict stock returns using market features
# Requirements:
# 1. Extract features from price data (technical indicators, volatility, etc.)
# 2. Add market regime features using clustering
# 3. Normalize features appropriately
# 4. Use LSTM or GRU neural network architecture
# 5. Implement walk-forward validation to prevent lookahead bias
# 6. Return model performance metrics and feature importance
</span>
<span class="c1"># Note: Follow our standard ML pipeline structure with preprocessing, 
# training, and evaluation steps clearly separated
</span>
<span class="k">def</span> <span class="nf">predict_stock_returns</span><span class="p">(</span>
    <span class="n">price_data</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">target_ticker</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prediction_horizon</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">features</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">walk_forward_window</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">252</span>
<span class="p">):</span>
    <span class="s">"""
    Build a model to predict future stock returns.
    
    Args:
        price_data: DataFrame with OHLCV data for multiple stocks
        target_ticker: Ticker symbol to predict
        prediction_horizon: Days ahead to predict
        features: List of features to use (None = use all available)
        test_size: Fraction of data to use for testing
        walk_forward_window: Number of days in each walk-forward window
        
    Returns:
        Tuple of (trained model, performance metrics dict, feature importance DataFrame)
    """</span>
    <span class="c1"># Let Copilot implement the function
</span></code></pre></div></div>

<hr />

<h2 id="10-troubleshooting-and-best-practices">10. Troubleshooting and Best Practices</h2>

<h3 id="when-copilot-generates-incorrect-code">When Copilot Generates Incorrect Code</h3>

<ol>
  <li><strong>Don’t accept it blindly</strong> - always review suggestions</li>
  <li><strong>Look for these common issues</strong>:
    <ul>
      <li>Off-by-one errors in loops or indices</li>
      <li>Incorrect API usage (especially with newer libraries)</li>
      <li>Unsafe assumptions about data structure</li>
      <li>Inefficient algorithms for large datasets</li>
      <li>Mixing precision types in financial calculations</li>
    </ul>
  </li>
  <li><strong>Use targeted comments</strong> to guide Copilot to a better solution:</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The previous suggestion had an off-by-one error
# Make sure to handle the case where index == len(prices) - 1
</span></code></pre></div></div>

<h3 id="maintaining-code-quality">Maintaining Code Quality</h3>

<ol>
  <li><strong>Run linters and type checkers</strong> on Copilot-generated code</li>
  <li><strong>Set up CI/CD pipelines</strong> that enforce coding standards</li>
  <li><strong>Schedule code reviews</strong> specifically for Copilot-assisted sections</li>
  <li><strong>Document which parts</strong> of your codebase were Copilot-assisted</li>
</ol>

<h3 id="security-considerations">Security Considerations</h3>

<ol>
  <li><strong>Always review generated code</strong> for security issues</li>
  <li><strong>Be cautious with data handling code</strong> - verify proper sanitization</li>
  <li><strong>Check for hardcoded credentials</strong> or sensitive information</li>
  <li><strong>Verify financial calculation accuracy</strong> - small errors can have massive impacts</li>
</ol>

<h3 id="best-practices-for-team-adoption">Best Practices for Team Adoption</h3>

<ol>
  <li><strong>Share effective prompts</strong> with team members</li>
  <li><strong>Document Copilot “wins”</strong> - cases where it significantly improved productivity</li>
  <li><strong>Maintain a shared PROMPT_TEMPLATES.md</strong> file with team-tested prompts</li>
  <li><strong>Hold regular “Copilot skill sharing” sessions</strong></li>
</ol>

<hr />

<h2 id="conclusion-your-journey-to-copilot-mastery">Conclusion: Your Journey to Copilot Mastery</h2>

<p>Becoming a GitHub Copilot ninja doesn’t happen overnight. It’s a journey of progressive skill development:</p>

<ol>
  <li><strong>Beginner</strong>: Using basic completions and accepting/rejecting suggestions</li>
  <li><strong>Intermediate</strong>: Writing effective comments and prompts to guide suggestions</li>
  <li><strong>Advanced</strong>: Implementing workflows that maximize context and guidance</li>
  <li><strong>Ninja</strong>: Combining all techniques to create a powerful development environment where Copilot becomes a true collaborator</li>
</ol>

<p>The techniques in this guide represent the current state-of-the-art for Copilot usage. As you apply them in your daily work, you’ll discover which ones work best for your specific tasks and coding style.</p>

<p>Remember that Copilot is a tool to enhance your capabilities, not replace your expertise. Your domain knowledge in financial analysis combined with these Copilot ninja techniques will dramatically increase your productivity and code quality.</p>

<p>Good luck on your Copilot ninja journey!</p>]]></content><author><name>Ben Reeve</name></author><category term="tutorials" /><summary type="html"><![CDATA[Executive Summary Welcome, Financial Analysts! This guide will transform you from GitHub Copilot users into GitHub Copilot ninjas. While Copilot is impressive out-of-the-box, its true potential is unlocked through strategic usage techniques that push its capabilities to the bleeding edge. Copilot works by generating code suggestions based on the context it sees. By default, it only “sees” your current file and perhaps a few open tabs. With the techniques in this guide, you’ll learn to: Feed Copilot the right context to generate better, more contextually aware code Create system-like prompts that guide Copilot to follow your project’s rules and patterns Leverage advanced techniques to simulate whole-project awareness Optimize Copilot for quantitative Python workflows Work around Copilot’s limitations with fast-evolving libraries and APIs This guide is designed for Python-focused analysts working in VS Code and Jupyter Notebooks. Whether you’re new to Copilot or already using it daily, these techniques will significantly boost your productivity and code quality. Table of Contents Setting Up Your Copilot Environment Feeding Context to Copilot Strategic Prompting: The Art of Guiding Copilot Simulating Whole-Project Awareness Optimizing for Python Quantitative Analysis Handling API Changes and Library Evolution Copilot Ninja Workflows Advanced Techniques: Beyond the Basics Practical Examples for Financial Analysis Troubleshooting and Best Practices 1. Setting Up Your Copilot Environment Basic Setup Install the GitHub Copilot extension in VS Code: Search for “GitHub Copilot” in the VS Code extensions marketplace Click Install Sign in with your GitHub account (contact your admin if you need access) Configure Copilot settings: Set your preferred keyboard shortcuts (Ctrl+Enter to accept suggestions) Enable inline suggestions (Editor &gt; Inlay Hints settings) Consider enabling the latest Copilot model if available (GPT-4 based model performs better) Jupyter Notebook Integration: VS Code has native support for .ipynb notebooks with Copilot Simply open your notebook files in VS Code - Copilot works automatically in code cells For those using JupyterLab, community extensions exist but VS Code provides the most seamless experience Enabling Copilot Chat (If Available) Copilot Chat adds conversational capabilities that regular Copilot doesn’t have: Install the GitHub Copilot Chat extension in VS Code Use the / commands in chat for specialized functions: /explain to understand complex code /fix to debug errors /tests to generate unit tests /workspace to ask questions about your entire project Optimal IDE Configuration Set up your VS Code environment to maximize Copilot’s effectiveness: Increase your editor font size: This gives Copilot more room to show inline suggestions Configure split views: Use split panes to keep relevant files visible simultaneously, as this improves Copilot’s context awareness Enable autosave: Helps Copilot learn from your edits quicker Use a consistent color theme: Dark themes often make Copilot’s ghost text more readable 2. Feeding Context to Copilot Copilot’s biggest limitation is its limited context window. By default, it only “sees” the file you’re currently editing and perhaps a few open tabs. Here’s how to feed it the right context: Open Relevant Files Keep related files open in your editor when working on a task. Copilot will access content from multiple open tabs when making suggestions: Module definitions: Keep imported module files open Configuration files: Open settings, config, or schema files Test files: Having tests open helps Copilot understand expected behavior Related code: Open files with similar functionality As confirmed by GitHub engineers: “if two files are open – one that defines a class, and another with unit tests for that class – by looking at both files, we have more context…resulting in higher-quality suggestions.” Creating Project Guide Documents Create dedicated guide files that Copilot can reference: Create a COPILOT_GUIDE.md or PROJECT_RULES.md containing: Architecture overview Tech stack details Coding style guidelines Performance constraints Common patterns in your codebase Example of a guide file: # Project Guide for Financial Analysis Codebase ## Architecture - We use a modular approach with these directories: - data/ - For data loading and preprocessing - models/ - For financial models and algorithms - viz/ - For visualization components - utils/ - For shared utility functions ## Tech Stack - Python 3.10+ - pandas 2.0+ for data manipulation - numpy for numerical operations - scikit-learn for ML components - matplotlib and seaborn for visualization ## Style Guidelines - We use PEP8 with 100 character line limit - Type hints are required for all function parameters and returns - Use descriptive variable names (no single-letter variables except in math formulas) - Document functions using Google-style docstrings - Prefer pandas vectorized operations over loops for performance ## Performance Constraints - Must handle datasets up to 10GB in memory - All financial calculations must use Decimal for precise money handling - Analysis functions should be optimizable for parallel processing Keep this guide open in a tab while coding to influence Copilot’s suggestions In-line Comment Directives Add strategic comments directly in your code to guide Copilot: Rule comments at the file top: # Copilot Instruction: This module uses numpy arrays for all data operations # Do not use Python lists for numerical calculations # Use vectorized operations rather than explicit loops # Use Decimal for all financial calculations requiring precision Function guidance comments: # Implement a function that calculates the covariance matrix # Must handle missing values gracefully # Should return a pandas DataFrame with labeled indices def calculate_covariance(data): # Copilot will now suggest implementation based on these constraints Architecture reminders: # This class follows the repository pattern # All database operations should be async # Must validate input and handle exceptions class MarketDataRepository: 3. Strategic Prompting: The Art of Guiding Copilot Prompting Copilot effectively is a skill that dramatically improves results: Write Comments Before Implementation Describe what you want before coding: # Function to normalize financial time series data # Takes a DataFrame with 'date' and 'price' columns # Performs the following: # 1. Calculates daily returns # 2. Handles missing values # 3. Winsorizes outliers (beyond 3 standard deviations) # 4. Scales values to zero mean and unit variance # 5. Returns the normalized DataFrame with original date index 8. Advanced Techniques: Beyond the Basics These techniques push Copilot to its limits: Use Repetitive Hooks Repeat important constraints in multiple places: # PROJECT RULE: All financial calculations must use Decimal for precision def calculate_compound_returns(prices): # Must use Decimal for precision # Implementation... # Ensure Decimal is used for all calculations before returning Multiple Suggestion Reviews Cycle through alternative completions before accepting: When Copilot suggests code, use Alt+] and Alt+[ to cycle through alternatives Or hover over the Copilot icon to see alternative suggestions Choose the suggestion that best matches your needs Maintain a “Prompt Template” File Create a PROMPT_TEMPLATES.md file with effective prompts for common tasks: # Copilot Prompt Templates ## For Data Loading Functions: # Function to load financial data from {format} file # Must handle errors gracefully with appropriate messages # Should return a pandas DataFrame with columns: {columns} # Performance requirement: Must handle files up to {size} GB ## For Visualization Functions: # Create a {plot_type} visualization showing {what_to_show} # X-axis represents {x_variable} # Y-axis represents {y_variable} # Use {color_scheme} color palette # Add proper labels, title, and legend # Make sure visualization is accessible (colorblind-friendly) Copy and customize these templates when needed PR-Driven Development Write detailed PR descriptions that explain changes across multiple files Reference these descriptions in your code comments Use the PR as a prompt for Copilot to understand the big picture Custom Instructions for Copilot Chat If your organization has enabled custom instructions for Copilot Chat: Create a .github/copilot/instructions.md file with project-specific guidance Include coding standards, architecture constraints, and common patterns These instructions will be injected into Copilot Chat’s context automatically 9. Practical Examples for Financial Analysis Let’s apply these techniques to common financial analysis tasks: Example 1: Market Data Processing Pipeline # Goal: Create a pipeline to process raw market data # Follow these project guidelines: # - Use pandas for data handling # - Implement efficient data cleaning # - Handle missing values, outliers, and data adjustments # - Calculate key technical indicators # - Store processed data in standardized format import os import pandas as pd import numpy as np from typing import List, Dict, Tuple, Optional from decimal import Decimal def process_market_data( ticker_list: List[str], start_date: str, end_date: str, indicators: List[str] = ["SMA", "EMA", "RSI", "MACD"] ) -&gt; pd.DataFrame: """ Process market data for multiple tickers and calculate technical indicators. Args: ticker_list: List of ticker symbols start_date: Start date in YYYY-MM-DD format end_date: End date in YYYY-MM-DD format indicators: List of technical indicators to calculate Returns: DataFrame with processed market data and indicators """ # Let Copilot implement the function Example 2: Portfolio Analysis with Visualization ## Analyzing Portfolio Performance Against Benchmarks We need to: 1. Load portfolio allocation and return data 2. Calculate key performance metrics (returns, volatility, Sharpe, drawdown) 3. Compare against market benchmarks (S&amp;P 500, sector indices) 4. Perform attribution analysis to identify sources of alpha 5. Visualize the results using appropriate plots # Implementation following the analysis plan above import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Dict, List # Load portfolio and benchmark data portfolio_returns = pd.read_csv('portfolio_returns.csv', index_col='date', parse_dates=True) benchmark_returns = pd.read_csv('benchmark_returns.csv', index_col='date', parse_dates=True) # Let Copilot continue implementing based on the markdown description Example 3: Machine Learning for Return Prediction # Implement a model to predict stock returns using market features # Requirements: # 1. Extract features from price data (technical indicators, volatility, etc.) # 2. Add market regime features using clustering # 3. Normalize features appropriately # 4. Use LSTM or GRU neural network architecture # 5. Implement walk-forward validation to prevent lookahead bias # 6. Return model performance metrics and feature importance # Note: Follow our standard ML pipeline structure with preprocessing, # training, and evaluation steps clearly separated def predict_stock_returns( price_data: pd.DataFrame, target_ticker: str, prediction_horizon: int = 5, features: List[str] = None, test_size: float = 0.2, walk_forward_window: int = 252 ): """ Build a model to predict future stock returns. Args: price_data: DataFrame with OHLCV data for multiple stocks target_ticker: Ticker symbol to predict prediction_horizon: Days ahead to predict features: List of features to use (None = use all available) test_size: Fraction of data to use for testing walk_forward_window: Number of days in each walk-forward window Returns: Tuple of (trained model, performance metrics dict, feature importance DataFrame) """ # Let Copilot implement the function 10. Troubleshooting and Best Practices When Copilot Generates Incorrect Code Don’t accept it blindly - always review suggestions Look for these common issues: Off-by-one errors in loops or indices Incorrect API usage (especially with newer libraries) Unsafe assumptions about data structure Inefficient algorithms for large datasets Mixing precision types in financial calculations Use targeted comments to guide Copilot to a better solution: # The previous suggestion had an off-by-one error # Make sure to handle the case where index == len(prices) - 1 Maintaining Code Quality Run linters and type checkers on Copilot-generated code Set up CI/CD pipelines that enforce coding standards Schedule code reviews specifically for Copilot-assisted sections Document which parts of your codebase were Copilot-assisted Security Considerations Always review generated code for security issues Be cautious with data handling code - verify proper sanitization Check for hardcoded credentials or sensitive information Verify financial calculation accuracy - small errors can have massive impacts Best Practices for Team Adoption Share effective prompts with team members Document Copilot “wins” - cases where it significantly improved productivity Maintain a shared PROMPT_TEMPLATES.md file with team-tested prompts Hold regular “Copilot skill sharing” sessions Conclusion: Your Journey to Copilot Mastery Becoming a GitHub Copilot ninja doesn’t happen overnight. It’s a journey of progressive skill development: Beginner: Using basic completions and accepting/rejecting suggestions Intermediate: Writing effective comments and prompts to guide suggestions Advanced: Implementing workflows that maximize context and guidance Ninja: Combining all techniques to create a powerful development environment where Copilot becomes a true collaborator The techniques in this guide represent the current state-of-the-art for Copilot usage. As you apply them in your daily work, you’ll discover which ones work best for your specific tasks and coding style. Remember that Copilot is a tool to enhance your capabilities, not replace your expertise. Your domain knowledge in financial analysis combined with these Copilot ninja techniques will dramatically increase your productivity and code quality. Good luck on your Copilot ninja journey!]]></summary></entry><entry><title type="html">Knowledge Companies Can’t Just Build Their Own ChatGPT</title><link href="http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html" rel="alternate" type="text/html" title="Knowledge Companies Can’t Just Build Their Own ChatGPT" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html"><![CDATA[<p><img src="/assets/images/posts/C67C6D79-61EA-4D25-969B-D0034B3E737B.png" alt="Knowledge Companies Can't Just Build Their Own ChatGPT" class="align-center" style="max-width: 100%;" /></p>

<h1 id="knowledge-companies-cant-just-build-their-own-chatgpt">Knowledge Companies Can’t Just Build Their Own ChatGPT</h1>

<p>The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h2 id="the-alignment-tax">The Alignment Tax</h2>

<p>Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget.</p>

<p>“But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference.</p>

<h2 id="the-tool-problem-is-even-worse">The Tool Problem Is Even Worse</h2>

<p>But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information.</p>

<p>This isn’t something you can replicate by writing some Python code to connect APIs. The model has been <em>trained through reinforcement learning</em> to understand <em>how</em> to use these tools effectively.</p>

<p>Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead.</p>

<p>This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results.</p>

<h2 id="knowledge-work-in-the-ai-age">Knowledge Work in the AI Age</h2>

<p>The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives.</p>

<p>That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.”</p>

<p>Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly:</p>
<ol>
  <li>Recognize knowledge gaps and seamlessly search for relevant case law</li>
  <li>Identify precedents warranting deeper exploration</li>
  <li>Pull data from multiple formats (legal databases, PDFs, tables)</li>
  <li>Synthesize contradictory rulings to provide a balanced analysis</li>
</ol>

<p>Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.”</p>

<p>The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates.</p>

<h2 id="goldman-sachs-does-not-want-to-be-told-its-proprietary-trading-algorithm-is-just-a-kid-in-a-hoodie">Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie</h2>

<p>This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives.</p>

<p>The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations.</p>

<p>Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in <em>Smith v. Jones</em>” feels less reassuring when the next version gets it right.</p>

<h2 id="the-three-options">The Three Options</h2>

<p>Knowledge economy companies essentially have three options, none particularly appealing:</p>

<p><strong>1. Partner with the leaders.</strong> Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.”</p>

<p><strong>2. Focus on proprietary data.</strong> The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives.</p>

<p><strong>3. Reframe the value proposition.</strong> Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters.</p>

<p>The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.”</p>

<h2 id="the-compounding-problem">The Compounding Problem</h2>

<p>In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving.</p>

<p>Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6.</p>

<p>This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment.</p>

<h2 id="why-this-matters">Why This Matters</h2>

<p>This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers.</p>

<p>It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value.</p>

<p>The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them.</p>

<p>Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research.</p>

<p>The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else.</p>

<p>In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is.</p>

<p>Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.”</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[Knowledge Companies Can’t Just Build Their Own ChatGPT The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.1 The Alignment Tax Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.2 This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget. “But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference. The Tool Problem Is Even Worse But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information. This isn’t something you can replicate by writing some Python code to connect APIs. The model has been trained through reinforcement learning to understand how to use these tools effectively. Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead. This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results. Knowledge Work in the AI Age The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives. That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.” Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly: Recognize knowledge gaps and seamlessly search for relevant case law Identify precedents warranting deeper exploration Pull data from multiple formats (legal databases, PDFs, tables) Synthesize contradictory rulings to provide a balanced analysis Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.” The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates. Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives. The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations. Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in Smith v. Jones” feels less reassuring when the next version gets it right. The Three Options Knowledge economy companies essentially have three options, none particularly appealing: 1. Partner with the leaders. Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.” 2. Focus on proprietary data. The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives. 3. Reframe the value proposition. Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters. The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.” The Compounding Problem In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving. Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6. This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment. Why This Matters This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers. It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value. The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them. Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research. The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else. In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is. Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.” Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. &#8617; Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. &#8617;]]></summary></entry><entry><title type="html">OpenAI Really Likes Reinforcement Learning</title><link href="http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html" rel="alternate" type="text/html" title="OpenAI Really Likes Reinforcement Learning" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html"><![CDATA[<p><img src="/assets/images/posts/818A0089-5B99-4717-8666-FF8FCC1DC6E0.png" alt="OpenAI Reinforcement Learning" class="align-center" style="max-width: 100%;" /></p>

<p>One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance.</p>

<h2 id="reinforcement-learning-is-everywhere-at-openai">Reinforcement Learning Is Everywhere at OpenAI</h2>

<p>OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea.</p>

<p>Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat.</p>

<p>But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!”</p>

<p>Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to.</p>

<p>The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.”</p>

<h2 id="the-distillation-vs-rl-debate">The Distillation vs. RL Debate</h2>

<p>There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach.</p>

<p>DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.”</p>

<p>The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance.</p>

<h2 id="ai-companies-are-playing-the-same-game-differently">AI Companies Are Playing the Same Game Differently</h2>

<p>What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish.</p>

<p>The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications.</p>

<h2 id="the-ai-restaurant-business">The AI Restaurant Business</h2>

<p>If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu.</p>

<p>DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens.</p>

<p>The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things.</p>

<p>And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole.</p>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance. Reinforcement Learning Is Everywhere at OpenAI OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea. Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat. But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!” Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to. The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.” The Distillation vs. RL Debate There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach. DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.” The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance. AI Companies Are Playing the Same Game Differently What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish. The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications. The AI Restaurant Business If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu. DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens. The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things. And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole.]]></summary></entry><entry><title type="html">The Great Divergence: How AI Will Transform Wealth Distribution in the 21st Century</title><link href="http://localhost:4000/industry-developments/2025/04/06/the-great-divergence.html" rel="alternate" type="text/html" title="The Great Divergence: How AI Will Transform Wealth Distribution in the 21st Century" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/the-great-divergence</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/the-great-divergence.html"><![CDATA[<p><img src="/assets/images/posts/91808461-E58F-4200-905F-90E65F88A7B7.png" alt="The Great Divergence" class="align-center" style="max-width: 100%;" /></p>

<div class="audio-player-container">
  <h3>Listen to this article</h3>
  <audio controls="">
    <source src="/assets/audio/The_Great_Divergence.mp3" type="audio/mpeg" />
    Your browser does not support the audio element.
  </audio>
</div>

<p>This is an audiobook version of my long-form article on AI and wealth inequality - a topic I am very passionate about. The views expressed are purely my own.</p>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[Listen to this article Your browser does not support the audio element. This is an audiobook version of my long-form article on AI and wealth inequality - a topic I am very passionate about. The views expressed are purely my own.]]></summary></entry><entry><title type="html">Accounting for AI</title><link href="http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html" rel="alternate" type="text/html" title="Accounting for AI" /><published>2025-04-06T00:00:00+01:00</published><updated>2025-04-06T00:00:00+01:00</updated><id>http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai</id><content type="html" xml:base="http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html"><![CDATA[<p><img src="/assets/images/posts/839B9CFA-91C9-4F23-B499-35988CB0D385.png" alt="Accounting for AI" class="align-center" style="max-width: 100%;" /></p>

<h1 id="accounting-for-ai">Accounting for AI</h1>

<p>There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources.</p>

<p>What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned.</p>

<p>But wait. Let’s read the footnotes.</p>

<p>DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development.</p>

<p>When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on:</p>

<ol>
  <li>Thousands of small-scale experiments to test approaches (think 1B-7B parameter models)</li>
  <li>Mid-sized validation runs to verify your approaches will scale</li>
  <li>Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data)</li>
  <li>Failed or abandoned approaches that didn’t make it into the final model</li>
</ol>

<p>Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different.</p>

<p>A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure.</p>

<h2 id="the-balance-sheet-matters">The Balance Sheet Matters</h2>

<p>There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car.</p>

<p>DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates.</p>

<h2 id="ai-economics-and-the-chip-geopolitics-premium">AI Economics and the Chip Geopolitics Premium</h2>

<p>There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions.</p>

<p>This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing.</p>

<p>The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense.</p>

<h2 id="open-source-or-open-weights">Open-Source or Open-Weights?</h2>

<p>DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense.</p>

<p>This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions.</p>

<p>There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated.</p>

<h2 id="the-frontier-ai-income-statement">The Frontier AI Income Statement</h2>

<p>So what does the real income statement of a frontier AI lab look like? Something like this:</p>

<ul>
  <li><strong>Revenue</strong>: API calls, enterprise licenses, VC funding (treated as revenue for startups)</li>
  <li><strong>COGS</strong>: GPU depreciation, electricity, cloud fees if applicable</li>
  <li><strong>R&amp;D</strong>: Engineering salaries, failed experiments, research computing</li>
  <li><strong>SG&amp;A</strong>: Executive salaries, offices, legal (especially important for AI safety compliance)</li>
  <li><strong>Adjusted EBITDA</strong>: Whatever number makes the business look sustainable</li>
  <li><strong>Actual Net Income</strong>: A number so negative it makes SoftBank investments look prudent</li>
</ul>

<p>When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.”</p>

<p>The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop.</p>

<p>But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice.</p>

<p>If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Reeve</name></author><category term="industry-developments" /><summary type="html"><![CDATA[Accounting for AI There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.1 The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources. What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned. But wait. Let’s read the footnotes. DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development. When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on: Thousands of small-scale experiments to test approaches (think 1B-7B parameter models) Mid-sized validation runs to verify your approaches will scale Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data) Failed or abandoned approaches that didn’t make it into the final model Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different. A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure. The Balance Sheet Matters There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car. DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates. AI Economics and the Chip Geopolitics Premium There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions. This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing. The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense. Open-Source or Open-Weights? DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense. This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions. There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated. The Frontier AI Income Statement So what does the real income statement of a frontier AI lab look like? Something like this: Revenue: API calls, enterprise licenses, VC funding (treated as revenue for startups) COGS: GPU depreciation, electricity, cloud fees if applicable R&amp;D: Engineering salaries, failed experiments, research computing SG&amp;A: Executive salaries, offices, legal (especially important for AI safety compliance) Adjusted EBITDA: Whatever number makes the business look sustainable Actual Net Income: A number so negative it makes SoftBank investments look prudent When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.” The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop. But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice. If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings. Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. &#8617;]]></summary></entry></feed>