<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css">
    <title>OpenAI Really Likes Reinforcement Learning</title><meta name="generator" content="Jekyll v3.9.5">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sometimes Models Just Do Things" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>OpenAI Really Likes Reinforcement Learning | Sometimes Models Just Do Things</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="OpenAI Really Likes Reinforcement Learning" />
<meta name="author" content="Ben Reeve" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance. Reinforcement Learning Is Everywhere at OpenAI OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea. Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat. But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!” Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to. The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.” The Distillation vs. RL Debate There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach. DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.” The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance. AI Companies Are Playing the Same Game Differently What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish. The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications. The AI Restaurant Business If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu. DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens. The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things. And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole." />
<meta property="og:description" content="One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance. Reinforcement Learning Is Everywhere at OpenAI OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea. Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat. But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!” Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to. The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.” The Distillation vs. RL Debate There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach. DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.” The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance. AI Companies Are Playing the Same Game Differently What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish. The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications. The AI Restaurant Business If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu. DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens. The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things. And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole." />
<link rel="canonical" href="http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html" />
<meta property="og:url" content="http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html" />
<meta property="og:site_name" content="Sometimes Models Just Do Things" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="OpenAI Really Likes Reinforcement Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Reeve"},"dateModified":"2025-04-06T00:00:00+01:00","datePublished":"2025-04-06T00:00:00+01:00","description":"One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance. Reinforcement Learning Is Everywhere at OpenAI OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea. Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat. But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!” Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to. The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.” The Distillation vs. RL Debate There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach. DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.” The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance. AI Companies Are Playing the Same Game Differently What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish. The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications. The AI Restaurant Business If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu. DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens. The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things. And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole.","headline":"OpenAI Really Likes Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html"},"url":"http://localhost:4000/industry-developments/2025/04/06/openai-really-likes-reinforcement-learning.html"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Sometimes Models Just Do Things</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/categories/industry-developments">Industry Developments</a>
        <a class="page-link" href="/categories/experiments">Experiments</a>
        <a class="page-link" href="/categories/tools">Tools</a>
        <a class="page-link" href="/categories/tutorials">Tutorials</a>
        <a class="page-link" href="/about/">About</a>
      </div>
    </nav>
  </div>
</header> <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="post-container">
    <header class="post-header">
      
      <div class="post-categories">
        
        <span class="post-category">industry-developments</span>
        
      </div>
      
      
      <h1 class="post-title p-name" itemprop="name headline">OpenAI Really Likes Reinforcement Learning</h1>
      
      <div class="post-meta">
        <time class="dt-published" datetime="2025-04-06T00:00:00+01:00" itemprop="datePublished">Apr 6, 2025
        </time>• <span class="post-author" itemprop="author">Ben Reeve</span></div>
    </header>

    <div class="post-content e-content" itemprop="articleBody">
      <p><img src="/assets/images/posts/818A0089-5B99-4717-8666-FF8FCC1DC6E0.png" alt="OpenAI Reinforcement Learning" class="align-center" style="max-width: 100%;" /></p>

<p>One way to think about artificial intelligence development is that companies build increasingly sophisticated machines to answer questions about how to build even more sophisticated machines. Another way to think about it is that everyone in AI is now obsessed with reinforcement learning. You know reinforcement learning: it’s the thing where you give the algorithm treats when it does what you want, like training a digital puppy. “Good AI, here’s a mathematical reward!” It turns out this approach is having quite the renaissance.</p>

<h2 id="reinforcement-learning-is-everywhere-at-openai">Reinforcement Learning Is Everywhere at OpenAI</h2>

<p>OpenAI seems to be putting reinforcement learning into everything these days. It’s like that friend who discovers kimchi and suddenly it’s in all their cooking. Sandwich? Add kimchi. Pasta? Kimchi. Ice cream? Well, you get the idea.</p>

<p>Their “o-series” reasoning models use reinforcement learning with verifiable rewards (RLVR). This is where they train models to get better at tasks where the answer can be unambiguously checked. If you ask an AI to solve 2+2 and it says “fish,” no reward. If it says “4,” it gets a mathematical treat.</p>

<p>But they didn’t stop there. Their “Operator” agent - the one that can use graphical interfaces like buttons and menus - also uses reinforcement learning. Here they’re basically teaching an AI to click on things by rewarding it when it successfully orders your burrito from DoorDash. “Good AI, you found the ‘extra guacamole’ button!”</p>

<p>Then there’s “Deep Research,” which uses reinforcement learning to get better at searching the web and consolidating information. And their GitHub Copilot model uses something called “reinforcement learning with code execution feedback” (RLEF), which I assume means the model gets a treat when its code actually compiles and does what it’s supposed to.</p>

<p>The most telling sign of OpenAI’s RL obsession? Their new o3-mini model has the same October 2023 knowledge cutoff as their other flagship models. They’re apparently so confident in their post-training methods - like RL - that they don’t even need to train on the most recent data. It’s like saying “our recipe is so good we don’t even need fresh ingredients anymore.”</p>

<h2 id="the-distillation-vs-rl-debate">The Distillation vs. RL Debate</h2>

<p>There’s an interesting debate happening in AI circles about whether “distillation” or “reinforcement learning” is the better approach for improving models. Distillation is where you train a smaller model to mimic a more powerful one, like having an apprentice chef learn from a master. RL is, well, the digital treat approach.</p>

<p>DeepSeek (another AI lab) made an interesting observation that while distillation works well, applying RL after distillation yields “significant further gains.” This is a bit like saying “training your apprentice chef by having them watch the master is good, but then also giving them treats when they make a good soufflé is even better.”</p>

<p>The tricky part apparently lies in “matching the data distribution” across different training phases. I’m not entirely sure what that means, but it sounds like making sure your apprentice chef is practicing on the same kinds of dishes the master excels at, before giving them treats for good performance.</p>

<h2 id="ai-companies-are-playing-the-same-game-differently">AI Companies Are Playing the Same Game Differently</h2>

<p>What’s fascinating here is how all these companies are basically working on the same core technologies but packaging them differently. OpenAI puts RL in everything from CoPilot to their Deep Research tool. They’re like a restaurant that puts their signature sauce on every dish.</p>

<p>The strategic implication seems to be that RL offers a path to “nearly perfect performance on a domain you can control carefully.” This is important because it suggests AI development isn’t just about having the biggest model or the most data - it’s about having the right training methods for specific applications.</p>

<h2 id="the-ai-restaurant-business">The AI Restaurant Business</h2>

<p>If we think about AI companies as restaurants, OpenAI seems to be running a chain where the secret isn’t just in having good ingredients (data) or a big kitchen (computing power), but in their cooking technique (reinforcement learning). They’ve found a way to make their dishes consistently taste good across their menu.</p>

<p>DeepSeek, meanwhile, is saying “hey, we should combine Julia Child’s techniques with Pavlovian conditioning for the absolute best results.” They haven’t shared their exact recipe, but they’re hinting that the combination is where the magic happens.</p>

<p>The really interesting part for the AI industry is that all of this suggests we’re moving from a world where raw power was everything to one where technique and domain expertise matter just as much. It’s not just about how big your model is, but how well you’ve trained it to do specific things.</p>

<p>And in the end, isn’t that what we all want? An AI that’s been given enough treats for ordering our burritos correctly that it never forgets the extra guacamole.</p>

    </div>

    <div class="post-navigation">
      <a href="/" class="back-to-blog">← Back to blog</a>
    </div>
  </div>
</article> 
      </div>
    </main><footer class="site-footer h-card">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h2 class="footer-heading">Sometimes Models Just Do Things</h2>
        <p>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</p>
      </div>

      <div class="footer-col footer-col-2">
        <h3>Categories</h3>
        <ul class="category-list">
          <li><a href="/categories/industry-developments">Industry Developments</a></li>
          <li><a href="/categories/experiments">Experiments</a></li>
          <li><a href="/categories/tools">Tools</a></li>
          <li><a href="/categories/tutorials">Tutorials</a></li>
        </ul>
      </div>
    </div>
    
    <div class="footer-bottom">
      <p>&copy; 2025 Ben Reeve</p>
    </div>
  </div>
</footer> </body>
</html> 