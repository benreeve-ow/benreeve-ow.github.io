<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css">
    <title>Knowledge Companies Can&#39;t Just Build Their Own ChatGPT</title><meta name="generator" content="Jekyll v3.9.5">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sometimes Models Just Do Things" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Knowledge Companies Can’t Just Build Their Own ChatGPT | Sometimes Models Just Do Things</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Knowledge Companies Can’t Just Build Their Own ChatGPT" />
<meta name="author" content="Ben Reeve" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Knowledge Companies Can’t Just Build Their Own ChatGPT The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.1 The Alignment Tax Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.2 This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget. “But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference. The Tool Problem Is Even Worse But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information. This isn’t something you can replicate by writing some Python code to connect APIs. The model has been trained through reinforcement learning to understand how to use these tools effectively. Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead. This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results. Knowledge Work in the AI Age The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives. That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.” Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly: Recognize knowledge gaps and seamlessly search for relevant case law Identify precedents warranting deeper exploration Pull data from multiple formats (legal databases, PDFs, tables) Synthesize contradictory rulings to provide a balanced analysis Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.” The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates. Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives. The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations. Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in Smith v. Jones” feels less reassuring when the next version gets it right. The Three Options Knowledge economy companies essentially have three options, none particularly appealing: 1. Partner with the leaders. Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.” 2. Focus on proprietary data. The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives. 3. Reframe the value proposition. Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters. The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.” The Compounding Problem In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving. Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6. This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment. Why This Matters This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers. It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value. The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them. Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research. The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else. In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is. Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.” Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. &#8617; Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. &#8617;" />
<meta property="og:description" content="Knowledge Companies Can’t Just Build Their Own ChatGPT The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.1 The Alignment Tax Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.2 This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget. “But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference. The Tool Problem Is Even Worse But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information. This isn’t something you can replicate by writing some Python code to connect APIs. The model has been trained through reinforcement learning to understand how to use these tools effectively. Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead. This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results. Knowledge Work in the AI Age The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives. That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.” Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly: Recognize knowledge gaps and seamlessly search for relevant case law Identify precedents warranting deeper exploration Pull data from multiple formats (legal databases, PDFs, tables) Synthesize contradictory rulings to provide a balanced analysis Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.” The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates. Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives. The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations. Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in Smith v. Jones” feels less reassuring when the next version gets it right. The Three Options Knowledge economy companies essentially have three options, none particularly appealing: 1. Partner with the leaders. Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.” 2. Focus on proprietary data. The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives. 3. Reframe the value proposition. Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters. The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.” The Compounding Problem In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving. Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6. This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment. Why This Matters This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers. It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value. The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them. Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research. The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else. In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is. Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.” Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. &#8617; Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. &#8617;" />
<link rel="canonical" href="http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html" />
<meta property="og:url" content="http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html" />
<meta property="og:site_name" content="Sometimes Models Just Do Things" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Knowledge Companies Can’t Just Build Their Own ChatGPT" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Reeve"},"dateModified":"2025-04-06T00:00:00+01:00","datePublished":"2025-04-06T00:00:00+01:00","description":"Knowledge Companies Can’t Just Build Their Own ChatGPT The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.1 The Alignment Tax Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.2 This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget. “But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference. The Tool Problem Is Even Worse But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information. This isn’t something you can replicate by writing some Python code to connect APIs. The model has been trained through reinforcement learning to understand how to use these tools effectively. Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead. This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results. Knowledge Work in the AI Age The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives. That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.” Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly: Recognize knowledge gaps and seamlessly search for relevant case law Identify precedents warranting deeper exploration Pull data from multiple formats (legal databases, PDFs, tables) Synthesize contradictory rulings to provide a balanced analysis Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.” The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates. Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives. The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations. Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in Smith v. Jones” feels less reassuring when the next version gets it right. The Three Options Knowledge economy companies essentially have three options, none particularly appealing: 1. Partner with the leaders. Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.” 2. Focus on proprietary data. The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives. 3. Reframe the value proposition. Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters. The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.” The Compounding Problem In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving. Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6. This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment. Why This Matters This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers. It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value. The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them. Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research. The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else. In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is. Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.” Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. &#8617; Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. &#8617;","headline":"Knowledge Companies Can’t Just Build Their Own ChatGPT","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html"},"url":"http://localhost:4000/industry-developments/2025/04/06/knowledge-companies-cant-just-build-their-own-chatgpt.html"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Sometimes Models Just Do Things</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/categories/industry-developments">Industry Developments</a>
        <a class="page-link" href="/categories/experiments">Experiments</a>
        <a class="page-link" href="/categories/tools">Tools</a>
        <a class="page-link" href="/categories/tutorials">Tutorials</a>
        <a class="page-link" href="/about/">About</a>
      </div>
    </nav>
  </div>
</header> <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="post-container">
    <header class="post-header">
      
      <div class="post-categories">
        
        <span class="post-category">industry-developments</span>
        
      </div>
      
      
      <h1 class="post-title p-name" itemprop="name headline">Knowledge Companies Can&#39;t Just Build Their Own ChatGPT</h1>
      
      <div class="post-meta">
        <time class="dt-published" datetime="2025-04-06T00:00:00+01:00" itemprop="datePublished">Apr 6, 2025
        </time>• <span class="post-author" itemprop="author">Ben Reeve</span></div>
    </header>

    <div class="post-content e-content" itemprop="articleBody">
      <p><img src="/assets/images/posts/C67C6D79-61EA-4D25-969B-D0034B3E737B.png" alt="Knowledge Companies Can't Just Build Their Own ChatGPT" class="align-center" style="max-width: 100%;" /></p>

<h1 id="knowledge-companies-cant-just-build-their-own-chatgpt">Knowledge Companies Can’t Just Build Their Own ChatGPT</h1>

<p>The thing about modern artificial intelligence is that it’s technically two things: there’s the raw intelligence part (the “large language model” or LLM), and then there’s the alignment part (making it actually do what humans want). This distinction matters quite a bit if you’re any knowledge economy company—a law firm, financial services provider, research organization, or professional services firm—thinking about building your own AI tools to compete with the likes of OpenAI, Anthropic or Google. The raw intelligence might be accessible, but the alignment—the part that makes AI tools actually useful—turns out to be astonishingly expensive.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h2 id="the-alignment-tax">The Alignment Tax</h2>

<p>Consider what Meta did with their LLaMA-2 chatbot. They spent something like $20-25 million just collecting human preference data—people saying “this response is better than that one”—to train their model to be helpful and not, you know, accidentally advise clients to try an exciting new tax strategy called “fraud.” This doesn’t include the hundreds of millions they spent on the base model training or the engineering talent. Meta calls this the “alignment tax,” which I love because it perfectly captures the grudging necessity of it all.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>This creates a kind of AI inequality that works exactly like financial inequality. The ultra-wealthy (in AI terms) have alignment strategies unavailable to the merely affluent. If you’re OpenAI, you can spend $100 million training GPT-4 and test 50 different versions before selecting a final candidate. Each of those test versions probably costs more than your entire law firm’s annual technology budget.</p>

<p>“But wait,” I hear you say, “Stanford researchers built Alpaca for only $600! Surely we can do something similar?” This is like saying “Warren Buffett started with a paper route, so I can definitely become a billionaire investor.” The $600 experiment was essentially having OpenAI’s models teach a smaller model how to behave. The results were, predictably, visibly worse than the teacher. Your clients, who are already using ChatGPT to draft initial contracts or analyze financial statements, will immediately notice the difference.</p>

<h2 id="the-tool-problem-is-even-worse">The Tool Problem Is Even Worse</h2>

<p>But here’s the really important part that’s easy to miss: When we talk about ChatGPT, we’re not just talking about GPT-4 as a standalone model. We’re talking about GPT-4 that has been specifically RL-trained to seamlessly integrate with tools—to know when to use a calculator, when to search the web, when to generate an image, and how to synthesize information.</p>

<p>This isn’t something you can replicate by writing some Python code to connect APIs. The model has been <em>trained through reinforcement learning</em> to understand <em>how</em> to use these tools effectively.</p>

<p>Take OpenAI’s research capabilities. They haven’t just connected their model to Google and called it a day. They’ve used reinforcement learning from human feedback to teach the model how to research: when to go deeper on a particular source, when to triangulate perspectives, when to dismiss an initially promising lead.</p>

<p>This is the difference between giving a Bloomberg terminal to a first-year analyst versus a 30-year market veteran. Same interface, entirely different results.</p>

<h2 id="knowledge-work-in-the-ai-age">Knowledge Work in the AI Age</h2>

<p>The professional services industry—law, finance, consulting, research—has long sold itself on having proprietary tools and methodologies. “Our unique analytical framework” is practically a required phrase in pitch decks. This has always been somewhat fictional—most professionals know the real value is in the people and the application of knowledge—but the fiction was sustainable because clients couldn’t easily access alternatives.</p>

<p>That’s changing. When your client can pay $20/month for ChatGPT Plus and get better legal research than your cobbled-together system produces, the premium positioning crumbles. It’s like finding out your hedge fund’s proprietary trading algorithm is actually just “buy tech stocks and charge 2 and 20.”</p>

<p>Consider what happens when a client asks a complex legal question. OpenAI’s system might invisibly:</p>
<ol>
  <li>Recognize knowledge gaps and seamlessly search for relevant case law</li>
  <li>Identify precedents warranting deeper exploration</li>
  <li>Pull data from multiple formats (legal databases, PDFs, tables)</li>
  <li>Synthesize contradictory rulings to provide a balanced analysis</li>
</ol>

<p>Your in-house system, meanwhile, will visibly struggle with transitions between these phases, often returning analysis that looks suspiciously like “we searched Westlaw and summarized the first page of results.”</p>

<p>The same applies to financial analysis. A bank’s proprietary AI might promise sophisticated market insights, but if it’s noticeably worse than what a client can access through a commercial API, the premium mystique evaporates.</p>

<h2 id="goldman-sachs-does-not-want-to-be-told-its-proprietary-trading-algorithm-is-just-a-kid-in-a-hoodie">Goldman Sachs Does Not Want To Be Told Its Proprietary Trading Algorithm Is Just A Kid In A Hoodie</h2>

<p>This situation has created fascinating corporate identity crises. Financial institutions that have spent decades branding themselves as technology companies now face the prospect of admitting they can’t build AI tools competitive with actual technology companies. Law firms that have invested millions in “proprietary legal tech” must confront the possibility that their custom-built systems are demonstrably inferior to publicly available alternatives.</p>

<p>The knowledge workers themselves are experiencing their own vertigo. The junior investment banker who spent three days building a DCF model watches ChatGPT produce a comparable analysis in 30 seconds. The associate attorney who spent a week researching precedents finds Claude has already compiled them, with citations.</p>

<p>Of course, they can point out flaws in the AI outputs—errors in the calculations, misinterpretations of precedents—but these feel increasingly like desperate rationalizations as the models improve. “It got the holding wrong in <em>Smith v. Jones</em>” feels less reassuring when the next version gets it right.</p>

<h2 id="the-three-options">The Three Options</h2>

<p>Knowledge economy companies essentially have three options, none particularly appealing:</p>

<p><strong>1. Partner with the leaders.</strong> Accept that building competitive AI tools requires resources beyond even large professional services firms, and negotiate special access to leading platforms. “We’ve integrated OpenAI’s technology with our proprietary data” isn’t as impressive as “we built our own AI,” but it’s better than “our AI is noticeably worse than what you’re already using.”</p>

<p><strong>2. Focus on proprietary data.</strong> The one area where firms might maintain advantage is in combining AI with truly proprietary data and frameworks. If you have exclusive data that general models can’t access, that’s valuable. This is why financial firms are frantically digitizing their historical trading data and law firms are racing to structure their case archives.</p>

<p><strong>3. Reframe the value proposition.</strong> Perhaps the premium isn’t in having better AI tools, but in having humans who know exactly when to use them and when to override them. This is the “AI whisperers” approach—we don’t have better hammers, but we have better carpenters.</p>

<p>The third option is probably the most realistic but requires a significant ego adjustment for firms that have traditionally positioned themselves as technology leaders. It’s hard to charge premium rates while admitting “we use the same tools as everyone else, just more skillfully.”</p>

<h2 id="the-compounding-problem">The Compounding Problem</h2>

<p>In a particularly cruel twist, this dynamic compounds over time. Every interaction with these RL-trained systems generates more training data to make them better. The models aren’t static—they’re constantly learning and improving.</p>

<p>Meanwhile, your static, manually-programmed tooling remains frozen at its initial capability level. The gap with state-of-the-art continuously widens, much like compound interest working against someone trying to catch up to the already-wealthy. By the time you’ve built something comparable to GPT-4’s capabilities, they’ll be on GPT-6.</p>

<p>This creates a kind of Darwinian pressure on knowledge work itself. Tasks that can be easily automated migrate to AI, while human professionals concentrate on work that resists automation—managing client relationships, handling novel situations, navigating ambiguity, and providing judgment.</p>

<h2 id="why-this-matters">Why This Matters</h2>

<p>This has broader implications for how value is created and captured in the knowledge economy. We’re seeing a pattern where companies with the resources to pay the “alignment tax” are capturing disproportionate value, while traditional knowledge firms are relegated to being implementation partners or value-added resellers.</p>

<p>It’s reminiscent of how cloud computing played out. Initially, many banks and law firms thought they needed to build their own secure cloud infrastructure. Most eventually realized that was economically irrational and shifted to using AWS with additional security layers. The winners weren’t those who tried to out-AWS Amazon, but those who most quickly accepted reality and found where they could add unique value.</p>

<p>The knowledge economy companies that will thrive aren’t those building second-rate AI tools, but those who most quickly adapt to a world where the best AI tools are widely available—and find ways to create value around them rather than competing with them.</p>

<p>Some hedge funds have already made this pivot successfully. Instead of trying to build better models than OpenAI, they’re focusing on identifying the unique data sources and judgment calls where humans still have edge. The best law firms are reimagining their services assuming clients already have access to basic AI legal research.</p>

<p>The alternative—pretending your homegrown AI is competitive when clients can directly compare it to ChatGPT—is a bit like claiming your in-house search engine is better than Google. It’s theoretically possible, but extraordinarily unlikely, and making the claim damages your credibility on everything else.</p>

<p>In the end, the knowledge economy isn’t being replaced by AI—it’s being reshaped by it. The value is shifting from information processing to judgment, from computation to wisdom, from tool-building to tool-wielding. That’s a much more profound change than simply adding AI to the existing business model, and it requires rethinking what knowledge work actually is.</p>

<p>Or as one hedge fund manager put it to me: “We’re not in the prediction business anymore. We’re in the judgment business. The models can predict. Humans still need to judge.”</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Large language models are basically systems trained to predict the next word in a sequence. Technically impressive, but left to their own devices, they’ll happily suggest that the SEC requires disclosure of executive astrological signs in 10-Ks. Alignment is the process of making them not do that. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Meta reportedly estimated their preference data collection alone cost millions. One analyst described it as the “cherry on top” that makes the model usable, but what a cherry—costing roughly 100,000 times more than the actual cherries you’d put on an ice cream sundae. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>

    <div class="post-navigation">
      <a href="/" class="back-to-blog">← Back to blog</a>
    </div>
  </div>
</article> 
      </div>
    </main><footer class="site-footer h-card">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h2 class="footer-heading">Sometimes Models Just Do Things</h2>
        <p>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</p>
      </div>

      <div class="footer-col footer-col-2">
        <h3>Categories</h3>
        <ul class="category-list">
          <li><a href="/categories/industry-developments">Industry Developments</a></li>
          <li><a href="/categories/experiments">Experiments</a></li>
          <li><a href="/categories/tools">Tools</a></li>
          <li><a href="/categories/tutorials">Tutorials</a></li>
        </ul>
      </div>
    </div>
    
    <div class="footer-bottom">
      <p>&copy; 2025 Ben Reeve</p>
    </div>
  </div>
</footer> </body>
</html> 