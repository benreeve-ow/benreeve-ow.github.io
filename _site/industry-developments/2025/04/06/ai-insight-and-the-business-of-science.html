<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css">
    <title>AI, Insight, and the Business of Science</title><meta name="generator" content="Jekyll v3.9.5">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sometimes Models Just Do Things" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AI, Insight, and the Business of Science | Sometimes Models Just Do Things</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="AI, Insight, and the Business of Science" />
<meta name="author" content="Ben Reeve" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas. This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape. The industrial processing of information AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports). This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable. But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems. This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries. Big Science™ and the acceleration paradox The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand. These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones. Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours. This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed. The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers. The institutional challenge Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints. Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically. Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation. This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers. The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research. Information vs. insight revisited This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish. This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage. The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation. And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide." />
<meta property="og:description" content="One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas. This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape. The industrial processing of information AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports). This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable. But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems. This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries. Big Science™ and the acceleration paradox The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand. These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones. Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours. This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed. The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers. The institutional challenge Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints. Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically. Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation. This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers. The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research. Information vs. insight revisited This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish. This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage. The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation. And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide." />
<link rel="canonical" href="http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html" />
<meta property="og:url" content="http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html" />
<meta property="og:site_name" content="Sometimes Models Just Do Things" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AI, Insight, and the Business of Science" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Reeve"},"dateModified":"2025-04-06T00:00:00+01:00","datePublished":"2025-04-06T00:00:00+01:00","description":"One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas. This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape. The industrial processing of information AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports). This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable. But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems. This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries. Big Science™ and the acceleration paradox The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand. These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones. Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours. This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed. The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers. The institutional challenge Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints. Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically. Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation. This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers. The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research. Information vs. insight revisited This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish. This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage. The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation. And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide.","headline":"AI, Insight, and the Business of Science","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html"},"url":"http://localhost:4000/industry-developments/2025/04/06/ai-insight-and-the-business-of-science.html"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Sometimes Models Just Do Things</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/categories/industry-developments">Industry Developments</a>
        <a class="page-link" href="/categories/experiments">Experiments</a>
        <a class="page-link" href="/categories/tools">Tools</a>
        <a class="page-link" href="/categories/tutorials">Tutorials</a>
        <a class="page-link" href="/about/">About</a>
      </div>
    </nav>
  </div>
</header> <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="post-container">
    <header class="post-header">
      
      <div class="post-categories">
        
        <span class="post-category">industry-developments</span>
        
      </div>
      
      
      <h1 class="post-title p-name" itemprop="name headline">AI, Insight, and the Business of Science</h1>
      
      <div class="post-meta">
        <time class="dt-published" datetime="2025-04-06T00:00:00+01:00" itemprop="datePublished">Apr 6, 2025
        </time>• <span class="post-author" itemprop="author">Ben Reeve</span></div>
    </header>

    <div class="post-content e-content" itemprop="articleBody">
      <p><img src="/assets/images/posts/4D3740B3-9D63-45A7-95A8-340E70094870.png" alt="AI and Science" class="align-center" style="max-width: 100%;" /></p>

<p>One thing to think about in artificial intelligence is the distinction between information processing and insight. AI models are getting really good at the former – they can process vast amounts of information, write coherent summaries, analyze patterns in data, and generate plausible content. But they struggle with the latter – having genuine “aha!” moments, making connections that weren’t implicit in their training data, or discovering truly novel ideas.</p>

<p>This isn’t a new observation, but it’s becoming increasingly relevant as AI tools like reasoning language models and research assistants mature and proliferate. These tools are tremendously useful for knowledge workers and researchers, but they highlight an interesting boundary in the cognitive landscape.</p>

<h2 id="the-industrial-processing-of-information">The industrial processing of information</h2>

<p>AI companies have been releasing increasingly sophisticated research tools that promise to revolutionize knowledge work. These systems can scan thousands of documents, synthesize information, and generate comprehensive reports with properly attributed sources. They’re essentially industrial-scale information processors – taking raw materials (papers, websites, data) and refining them into finished goods (summaries, analyses, reports).</p>

<p>This is genuinely useful! Knowledge workers spend enormous amounts of time gathering, organizing, and synthesizing information. Scientists pore through literature reviews, programmers wade through documentation, analysts compile market research. Having machines that can automate large chunks of this work is valuable.</p>

<p>But there’s something these systems can’t yet do, which is to have genuine insights – those moments of connection and creativity that lead to scientific breakthroughs or novel theories. As one commentator cleverly put it, “To an LLM, a novel discovery is indistinguishable from an error.” That’s because language models fundamentally work by predicting what should come next based on patterns they’ve seen before. A truly novel insight – one that departs from existing patterns – would look like a mistake to such systems.</p>

<p>This creates an interesting economic dynamic. The marginal cost of information processing is plummeting toward zero, while the value of genuine insight remains high. This shift will likely reshape labor markets, business models, and competitive landscapes across knowledge-intensive industries.</p>

<h2 id="big-science-and-the-acceleration-paradox">Big Science™ and the acceleration paradox</h2>

<p>The conventional approach to “AI for Science” has been to build specialized systems for particular scientific domains – things like protein folding, mathematical proofs, or weather prediction. These projects typically combine general AI capabilities (like deep learning and transformer architectures) with domain-specific features tailored to the scientific problem at hand.</p>

<p>These specialized systems have been impressively successful, but they’re not really doing science in the way humans do it. They’re more akin to extremely sophisticated measurement instruments or simulation tools – they extend our capabilities within existing paradigms rather than creating new ones.</p>

<p>Meanwhile, the general-purpose AI tools – coding assistants, reasoning models, research systems – are dramatically accelerating the pace of normal scientific work. They’re reducing the time required to implement ideas, analyze results, and communicate findings. In some fields, particularly those heavy on computation, they’re compressing what might have been months of work into days or hours.</p>

<p>This creates what we might call an acceleration paradox: the institutional structures of science (peer review, academic publishing, grant funding) operate on timescales of months to years, but the actual work of science is accelerating to timescales of days to weeks. It’s like having a Ferrari on a road with a 25 mph speed limit – the infrastructure constrains the potential speed.</p>

<p>The business implications are fascinating. If scientific progress accelerates but publications lag, we might see companies capturing more of the value of scientific advances before they become public knowledge. The advantage will go to organizations that can rapidly integrate new findings into products and services, rather than those that publish prestigious papers.</p>

<h2 id="the-institutional-challenge">The institutional challenge</h2>

<p>Scientific institutions evolved in an era of information scarcity and high communication costs. They were designed to curate, validate, and disseminate knowledge in a world where those functions were difficult and expensive. But AI is rapidly changing those underlying constraints.</p>

<p>Consider the PhD – traditionally representing roughly 4-6 years of specialized training and original research. But what happens when AI tools can compress much of that work into a fraction of the time? The credential itself doesn’t change, but the relative value of what it represents shifts dramatically.</p>

<p>Or think about peer review, which typically takes months. In a world where research can be conducted in days, this creates enormous friction in the system. There’s a growing mismatch between the pace of discovery and the pace of validation.</p>

<p>This mismatch creates obvious business opportunities. Companies that can build alternative validation mechanisms – perhaps using AI to pre-check research for errors, inconsistencies, or plagiarism – could capture significant value. Platforms that enable faster dissemination and collaboration around scientific findings could displace traditional publishers.</p>

<p>The challenge here isn’t technological – it’s institutional. Academic incentives, funding mechanisms, and career paths all evolved in the pre-AI era. Updating these to match the new technological reality will be complex and contentious. The institutions that adapt most effectively will likely attract the best talent and produce the most valuable research.</p>

<h2 id="information-vs-insight-revisited">Information vs. insight revisited</h2>

<p>This all circles back to the fundamental tension between information processing (which AI excels at) and insight generation (which remains primarily human). The most successful scientific enterprises will likely be those that effectively combine the two – using AI to handle the information-intensive aspects of research while creating environments where human insight can flourish.</p>

<p>This isn’t just about building better AI tools; it’s about redesigning workflows, incentives, and organizations to leverage the complementary strengths of humans and machines. Companies that figure this out first will have a significant competitive advantage.</p>

<p>The future of science isn’t humans versus AI, or even humans with AI. It’s more likely humans with AI versus other humans with AI, competing to design systems that maximize the generation and application of new knowledge. The winners will be those who understand not just the technology, but the underlying human processes of discovery and innovation.</p>

<p>And that understanding – ironically enough – requires exactly the kind of insight that AI still struggles to provide.</p>

    </div>

    <div class="post-navigation">
      <a href="/" class="back-to-blog">← Back to blog</a>
    </div>
  </div>
</article> 
      </div>
    </main><footer class="site-footer h-card">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h2 class="footer-heading">Sometimes Models Just Do Things</h2>
        <p>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</p>
      </div>

      <div class="footer-col footer-col-2">
        <h3>Categories</h3>
        <ul class="category-list">
          <li><a href="/categories/industry-developments">Industry Developments</a></li>
          <li><a href="/categories/experiments">Experiments</a></li>
          <li><a href="/categories/tools">Tools</a></li>
          <li><a href="/categories/tutorials">Tutorials</a></li>
        </ul>
      </div>
    </div>
    
    <div class="footer-bottom">
      <p>&copy; 2025 Ben Reeve</p>
    </div>
  </div>
</footer> </body>
</html> 