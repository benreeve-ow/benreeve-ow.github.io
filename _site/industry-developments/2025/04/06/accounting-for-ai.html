<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css">
    <title>Accounting for AI</title><meta name="generator" content="Jekyll v3.9.5">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sometimes Models Just Do Things" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Accounting for AI | Sometimes Models Just Do Things</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Accounting for AI" />
<meta name="author" content="Ben Reeve" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Accounting for AI There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.1 The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources. What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned. But wait. Let’s read the footnotes. DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development. When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on: Thousands of small-scale experiments to test approaches (think 1B-7B parameter models) Mid-sized validation runs to verify your approaches will scale Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data) Failed or abandoned approaches that didn’t make it into the final model Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different. A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure. The Balance Sheet Matters There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car. DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates. AI Economics and the Chip Geopolitics Premium There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions. This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing. The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense. Open-Source or Open-Weights? DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense. This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions. There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated. The Frontier AI Income Statement So what does the real income statement of a frontier AI lab look like? Something like this: Revenue: API calls, enterprise licenses, VC funding (treated as revenue for startups) COGS: GPU depreciation, electricity, cloud fees if applicable R&amp;D: Engineering salaries, failed experiments, research computing SG&amp;A: Executive salaries, offices, legal (especially important for AI safety compliance) Adjusted EBITDA: Whatever number makes the business look sustainable Actual Net Income: A number so negative it makes SoftBank investments look prudent When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.” The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop. But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice. If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings. Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. &#8617;" />
<meta property="og:description" content="Accounting for AI There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.1 The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources. What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned. But wait. Let’s read the footnotes. DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development. When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on: Thousands of small-scale experiments to test approaches (think 1B-7B parameter models) Mid-sized validation runs to verify your approaches will scale Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data) Failed or abandoned approaches that didn’t make it into the final model Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different. A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure. The Balance Sheet Matters There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car. DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates. AI Economics and the Chip Geopolitics Premium There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions. This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing. The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense. Open-Source or Open-Weights? DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense. This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions. There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated. The Frontier AI Income Statement So what does the real income statement of a frontier AI lab look like? Something like this: Revenue: API calls, enterprise licenses, VC funding (treated as revenue for startups) COGS: GPU depreciation, electricity, cloud fees if applicable R&amp;D: Engineering salaries, failed experiments, research computing SG&amp;A: Executive salaries, offices, legal (especially important for AI safety compliance) Adjusted EBITDA: Whatever number makes the business look sustainable Actual Net Income: A number so negative it makes SoftBank investments look prudent When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.” The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop. But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice. If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings. Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. &#8617;" />
<link rel="canonical" href="http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html" />
<meta property="og:url" content="http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html" />
<meta property="og:site_name" content="Sometimes Models Just Do Things" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Accounting for AI" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Reeve"},"dateModified":"2025-04-06T00:00:00+01:00","datePublished":"2025-04-06T00:00:00+01:00","description":"Accounting for AI There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.1 The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources. What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned. But wait. Let’s read the footnotes. DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development. When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on: Thousands of small-scale experiments to test approaches (think 1B-7B parameter models) Mid-sized validation runs to verify your approaches will scale Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data) Failed or abandoned approaches that didn’t make it into the final model Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different. A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure. The Balance Sheet Matters There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car. DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates. AI Economics and the Chip Geopolitics Premium There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions. This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing. The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense. Open-Source or Open-Weights? DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense. This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions. There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated. The Frontier AI Income Statement So what does the real income statement of a frontier AI lab look like? Something like this: Revenue: API calls, enterprise licenses, VC funding (treated as revenue for startups) COGS: GPU depreciation, electricity, cloud fees if applicable R&amp;D: Engineering salaries, failed experiments, research computing SG&amp;A: Executive salaries, offices, legal (especially important for AI safety compliance) Adjusted EBITDA: Whatever number makes the business look sustainable Actual Net Income: A number so negative it makes SoftBank investments look prudent When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.” The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop. But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice. If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings. Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. &#8617;","headline":"Accounting for AI","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html"},"url":"http://localhost:4000/industry-developments/2025/04/06/accounting-for-ai.html"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Sometimes Models Just Do Things</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/categories/industry-developments">Industry Developments</a>
        <a class="page-link" href="/categories/experiments">Experiments</a>
        <a class="page-link" href="/categories/tools">Tools</a>
        <a class="page-link" href="/categories/tutorials">Tutorials</a>
        <a class="page-link" href="/about/">About</a>
      </div>
    </nav>
  </div>
</header> <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="post-container">
    <header class="post-header">
      
      <div class="post-categories">
        
        <span class="post-category">industry-developments</span>
        
      </div>
      
      
      <h1 class="post-title p-name" itemprop="name headline">Accounting for AI</h1>
      
      <div class="post-meta">
        <time class="dt-published" datetime="2025-04-06T00:00:00+01:00" itemprop="datePublished">Apr 6, 2025
        </time>• <span class="post-author" itemprop="author">Ben Reeve</span></div>
    </header>

    <div class="post-content e-content" itemprop="articleBody">
      <p><img src="/assets/images/posts/839B9CFA-91C9-4F23-B499-35988CB0D385.png" alt="Accounting for AI" class="align-center" style="max-width: 100%;" /></p>

<h1 id="accounting-for-ai">Accounting for AI</h1>

<p>There’s a thing that happens in the world of frontier AI models where companies announce how much it cost to train their latest model, and everyone gets excited about the falling costs of artificial intelligence. “Only $X million to train a state-of-the-art model!” the headlines proclaim, and people start updating their spreadsheets about AI democratization timelines. But these numbers have a certain “adjusted EBITDA” quality to them. They’re not exactly… lies? But they’re certainly selective presentations of cost structures that wouldn’t pass muster in an SEC filing.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>The latest example comes from DeepSeek AI, a Chinese AI company that recently released DeepSeek-V3. It’s a mixture of experts (MoE) model with 671 billion total parameters but only 37 billion “active” parameters at any given time. Think of it as having 671 billion neurons but only letting 37 billion of them fire simultaneously - a kind of neural timeshare that keeps the compute requirements manageable. The model apparently performs impressively against competitors like Meta’s Llama 405B while using far less computational resources.</p>

<p>What got everyone excited was DeepSeek’s claim that training this model required only 2.6 million GPU hours, compared to Meta’s 30.8 million GPU hours for Llama 3. This efficiency supposedly brought the training cost down to around $5.5 million. “AI is getting so cheap!” everyone exclaimed, and tech Twitter collectively swooned.</p>

<p>But wait. Let’s read the footnotes.</p>

<p>DeepSeek acknowledges that this figure “includes only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.” That’s a bit like Apple saying the iPhone 16 only cost $10 million to manufacture, if you don’t count the R&amp;D, failed prototypes, or software development.</p>

<p>When you’re building frontier AI models, the final training run is just the tip of the iceberg. Before that, you’ve likely spent vastly more compute on:</p>

<ol>
  <li>Thousands of small-scale experiments to test approaches (think 1B-7B parameter models)</li>
  <li>Mid-sized validation runs to verify your approaches will scale</li>
  <li>Related models that provide capabilities or data for your main model (DeepSeek used their R1 model for synthetic data)</li>
  <li>Failed or abandoned approaches that didn’t make it into the final model</li>
</ol>

<p>Then there’s the human cost. DeepSeek’s paper lists 139 technical authors. Even with China’s lower labor costs, that’s a substantial payroll. Add in infrastructure costs like electricity (which can exceed $10 million annually for large GPU clusters), cooling, networking equipment, and the capital expenditure on the GPUs themselves, and the real cost starts looking very different.</p>

<p>A more realistic estimate might be $500 million to $1 billion annually for a company operating at DeepSeek’s scale - not exactly the democratized AI future some were envisioning after seeing that $5.5 million figure.</p>

<h2 id="the-balance-sheet-matters">The Balance Sheet Matters</h2>

<p>There’s also an interesting accounting distinction here between capital expenditures and operating expenses. The $5.5 million figure describes the marginal cost of one training run using already-purchased hardware. But that’s like saying it only costs $50 in petrol to drive a Ferrari from New York to Boston, ignoring the $300,000 you spent buying the car.</p>

<p>DeepSeek is reportedly using H800 GPUs, which are “nerfed” versions of Nvidia’s H100s created specifically for the Chinese market to comply with U.S. export controls. These GPUs have some limitations on communication speeds between chips, but their raw computing power remains largely intact. Various estimates put DeepSeek’s GPU inventory at somewhere between 20,000 and 50,000 A100 equivalents - a substantial asset that would cost billions to acquire at market rates.</p>

<h2 id="ai-economics-and-the-chip-geopolitics-premium">AI Economics and the Chip Geopolitics Premium</h2>

<p>There’s another subplot here about U.S.-China tech competition. DeepSeek’s narrative of “look how much more we can do with less” makes strategic sense for a Chinese company operating under chip export controls. It’s a flex aimed at both recruiting talent and demonstrating resilience against Western sanctions.</p>

<p>This is similar to how startups with tough unit economics emphasize their growth rate rather than their burn rate. “Don’t look at our massive losses, look at our customer acquisition velocity!” When your access to computing resources is constrained by geopolitics, emphasizing efficiency becomes not just good engineering but good marketing.</p>

<p>The irony is that DeepSeek has likely spent more absolute dollars on this model than they’re letting on, precisely because they have to work around chip export controls. The H800s they’re using cost more on the secondary market, and the engineering effort required to optimize for their limitations adds another layer of expense.</p>

<h2 id="open-source-or-open-weights">Open-Source or Open-Weights?</h2>

<p>DeepSeek released their model under an open-weights license, meaning you can download and use the parameters, but you don’t get the training code or data. That’s like giving someone a compiled program without the source code - useful, but not exactly “open source” in the traditional sense.</p>

<p>This distinction matters for the cost narrative. If DeepSeek had released everything - training data, code, infrastructure optimizations - then others really could replicate their work for closer to that $5.5 million figure. But without those components, anyone wanting to build something similar would need to reinvent much of what DeepSeek built, pushing the real cost back toward the hundreds of millions.</p>

<p>There’s a pattern in AI where companies release “open” models with just enough information to be useful but not enough to be truly replicable. It’s a bit like investment banks releasing “research” that conveniently supports their trading positions - helpful but carefully curated.</p>

<h2 id="the-frontier-ai-income-statement">The Frontier AI Income Statement</h2>

<p>So what does the real income statement of a frontier AI lab look like? Something like this:</p>

<ul>
  <li><strong>Revenue</strong>: API calls, enterprise licenses, VC funding (treated as revenue for startups)</li>
  <li><strong>COGS</strong>: GPU depreciation, electricity, cloud fees if applicable</li>
  <li><strong>R&amp;D</strong>: Engineering salaries, failed experiments, research computing</li>
  <li><strong>SG&amp;A</strong>: Executive salaries, offices, legal (especially important for AI safety compliance)</li>
  <li><strong>Adjusted EBITDA</strong>: Whatever number makes the business look sustainable</li>
  <li><strong>Actual Net Income</strong>: A number so negative it makes SoftBank investments look prudent</li>
</ul>

<p>When DeepSeek or any other AI lab announces training costs, they’re essentially reporting an adjusted non-GAAP metric that ignores most of these categories. It’s the AI equivalent of “we lose money on each unit but make it up in volume.”</p>

<p>The good news is that these costs really are falling over time. The learning efficiency of models continues to improve through innovations like DeepSeek’s multi-head latent attention, multi-token prediction, and 8-bit native training. In a few years, frontier models might genuinely cost $5-10 million all-in to develop.</p>

<p>But for now, these headline numbers should be treated like crypto whitepaper projections - theoretically possible under ideal conditions that never seem to materialize in practice.</p>

<p>If someone tells you they built a frontier AI model for a few million dollars, ask them if that includes the cost of hiring 139 engineers. If not, you might be looking at AI’s version of adjusted, non-GAAP, excluding-stock-based-compensation earnings.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Though who knows, maybe they would. We live in a world where companies can go public with “community adjusted EBITDA” metrics that exclude most of their actual expenses. At least AI labs aren’t publicly traded yet. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>

    <div class="post-navigation">
      <a href="/" class="back-to-blog">← Back to blog</a>
    </div>
  </div>
</article> 
      </div>
    </main><footer class="site-footer h-card">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h2 class="footer-heading">Sometimes Models Just Do Things</h2>
        <p>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</p>
      </div>

      <div class="footer-col footer-col-2">
        <h3>Categories</h3>
        <ul class="category-list">
          <li><a href="/categories/industry-developments">Industry Developments</a></li>
          <li><a href="/categories/experiments">Experiments</a></li>
          <li><a href="/categories/tools">Tools</a></li>
          <li><a href="/categories/tutorials">Tutorials</a></li>
        </ul>
      </div>
    </div>
    
    <div class="footer-bottom">
      <p>&copy; 2025 Ben Reeve</p>
    </div>
  </div>
</footer> </body>
</html> 