<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css">
    <title>Google&#39;s Second Chance at AI Dominance</title><meta name="generator" content="Jekyll v3.9.5">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sometimes Models Just Do Things" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Google’s Second Chance at AI Dominance | Sometimes Models Just Do Things</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Google’s Second Chance at AI Dominance" />
<meta name="author" content="Ben Reeve" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book. Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.1 This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023. The Business of Benchmarks It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.2 The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush. Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access. The Multimodal Moat Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system. The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.3 The VC-to-Viability Pipeline For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen. This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.4 Pricing: The Great Unknown The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost. If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem. The Future of AI Thinking Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio. The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus. Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales. But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle. Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. &#8617; There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. &#8617; This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” &#8617; For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. &#8617;" />
<meta property="og:description" content="One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book. Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.1 This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023. The Business of Benchmarks It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.2 The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush. Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access. The Multimodal Moat Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system. The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.3 The VC-to-Viability Pipeline For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen. This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.4 Pricing: The Great Unknown The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost. If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem. The Future of AI Thinking Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio. The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus. Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales. But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle. Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. &#8617; There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. &#8617; This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” &#8617; For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. &#8617;" />
<link rel="canonical" href="http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html" />
<meta property="og:url" content="http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html" />
<meta property="og:site_name" content="Sometimes Models Just Do Things" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T15:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Google’s Second Chance at AI Dominance" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Reeve"},"dateModified":"2025-04-06T15:00:00+01:00","datePublished":"2025-04-06T15:00:00+01:00","description":"One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book. Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.1 This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023. The Business of Benchmarks It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.2 The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush. Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access. The Multimodal Moat Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system. The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.3 The VC-to-Viability Pipeline For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen. This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.4 Pricing: The Great Unknown The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost. If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem. The Future of AI Thinking Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio. The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus. Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales. But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle. Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. &#8617; There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. &#8617; This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” &#8617; For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. &#8617;","headline":"Google’s Second Chance at AI Dominance","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html"},"url":"http://localhost:4000/industry-developments/2025/04/06/googles-second-chance-at-ai-dominance.html"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Sometimes Models Just Do Things</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/categories/industry-developments">Industry Developments</a>
        <a class="page-link" href="/categories/experiments">Experiments</a>
        <a class="page-link" href="/categories/tools">Tools</a>
        <a class="page-link" href="/categories/tutorials">Tutorials</a>
        <a class="page-link" href="/about/">About</a>
      </div>
    </nav>
  </div>
</header> <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="post-container">
    <header class="post-header">
      
      <div class="post-categories">
        
        <span class="post-category">industry-developments</span>
        
      </div>
      
      
      <h1 class="post-title p-name" itemprop="name headline">Google&#39;s Second Chance at AI Dominance</h1>
      
      <div class="post-meta">
        <time class="dt-published" datetime="2025-04-06T15:00:00+01:00" itemprop="datePublished">Apr 6, 2025
        </time>• <span class="post-author" itemprop="author">Ben Reeve</span></div>
    </header>

    <div class="post-content e-content" itemprop="articleBody">
      <p><img src="/assets/images/posts/458D04C0-8CE5-4CA5-B088-071F1E171B5E_4_5005_c.jpeg" alt="Google AI" class="align-center" style="max-width: 100%; margin: 20px auto;" /></p>

<p>One of the peculiar narratives in modern technology is how Google, with its vast resources, elite talent pool, and decade-long head start in machine learning, somehow found itself playing catch-up in the generative AI revolution. It’s like watching the world’s greatest chess player lose to a novice because they were distracted reading a book about chess strategy. But it seems Google has finally looked up from its book.</p>

<p>Google announced Gemini 2.5 Pro, and by all accounts, it’s not just incrementally better than competing models—it’s leaping ahead. The model is reportedly outperforming competitors by more than 40 Elo points on the ChatBotArena benchmark, which in this world is like showing up to a knife fight with a tactical nuclear weapon.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>This isn’t just a small step forward in a gradual progression. This appears to be the second-largest jump in top model performance in the history of the LMSYS leaderboard, behind only when GPT-4 Turbo surpassed Claude 1. Of course, that earlier jump happened before companies really understood they were competing on benchmarks, back in the prehistoric days of 2023.</p>

<h2 id="the-business-of-benchmarks">The Business of Benchmarks</h2>

<p>It’s worth pausing to consider the strange economics of AI model benchmarks. Companies invest billions in training these models, and then we measure their success with leaderboards that somewhat resemble video game high-score tables. There’s something delightfully absurd about trillion-dollar companies competing for position on volunteer-run evaluation platforms.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>The reason we care about benchmarks is that they’re our best proxy for model capabilities. And capabilities drive adoption, which drives revenue, which drives more investment in capabilities. It’s the circle of AI life, playing out in quarterly increments of compute spend that would make a small nation’s GDP blush.</p>

<p>Google’s performance on reasoning-focused benchmarks is particularly noteworthy. They’re reporting a score of 18.8 on “Humanity’s Last Exam” without search or tools, which is remarkable considering that just months ago, OpenAI was touting its Deep Research as groundbreaking for being able to tackle this kind of complex reasoning—and that required web access.</p>

<h2 id="the-multimodal-moat">The Multimodal Moat</h2>

<p>Google isn’t just beating others on language benchmarks. They’re maintaining advantages in multimodal capabilities (including audio) and context length. This is the AI equivalent of not just having the fastest car but also the most comfortable seats and the best sound system.</p>

<p>The technical explanation from Google is predictably vague: “a significantly enhanced base model with improved post-training.” This is like a chef describing their award-winning dish as “better ingredients, better cooking.” Technically accurate but deliberately uninformative.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<h2 id="the-vc-to-viability-pipeline">The VC-to-Viability Pipeline</h2>

<p>For AI startups caught in the middle of this frontier model arms race, the competitive landscape just got more challenging. As Google, OpenAI, and others push state-of-the-art performance further, the gap between what’s possible with proprietary models and what’s available to startups via open-source alternatives continues to widen.</p>

<p>This creates a strange dynamic where venture capital pours into AI startups that are fundamentally dependent on technology controlled by the very tech giants they’re competing against. It’s like opening a restaurant where you have to buy all your ingredients from a competitor’s grocery store.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p>

<h2 id="pricing-the-great-unknown">Pricing: The Great Unknown</h2>

<p>The article notably mentions that until we have API pricing, it’s difficult to make informed guesses about whether Gemini 2.5 Pro is a massive model like GPT-4.5. This highlights one of the fundamental tensions in the AI market: the relationship between model size, performance, and cost.</p>

<p>If Google can deliver superior performance at a lower cost than competitors, they could disrupt the current pricing equilibrium. If, however, they charge similar prices to OpenAI, then competition shifts to other dimensions like reliability, integration, and ecosystem.</p>

<h2 id="the-future-of-ai-thinking">The Future of AI Thinking</h2>

<p>Perhaps most revealing is Google’s statement that they’re “building these thinking capabilities directly into all of our models.” This suggests a strategic shift away from specialized reasoning models toward making advanced reasoning a standard feature across their AI portfolio.</p>

<p>The irony here is that Google, which pioneered large-scale neural networks and transformer architectures that made modern AI possible, is now playing catch-up by enhancing the very technology they helped create. It’s like watching the inventor of the automobile sprint to catch a bus.</p>

<p>Will Gemini 2.5 Pro be enough to reestablish Google’s AI leadership? The benchmarks suggest yes, but the market is more complicated. OpenAI has first-mover advantage, Microsoft has distribution, and Anthropic has that ineffable quality of being not-Google and not-OpenAI, which counts for something in enterprise sales.</p>

<p>But the biggest winners in this AI arms race might be the customers, who get increasingly capable models to play with while the tech giants compete for dominance. It’s like watching Godzilla fight King Kong while you collect the gold coins they shake loose during the battle.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Elo ratings, originally developed for chess, measure relative skill levels. A difference of 40 points means the higher-rated model would be expected to win about 57% of the time, which in the near-perfect-information world of AI benchmarks is actually quite significant. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>There’s probably a dissertation waiting to be written about how volunteer-run benchmarks have become the de facto standard for measuring progress in a trillion-dollar industry. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This vagueness is strategic. Google wants to telegraph its superiority without giving competitors a roadmap to replicate their approach. In the AI world, this is called “capabilities demonstration without capabilities transfer.” <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>For all the talk about AI democratization, the reality is that the most capable models remain controlled by a small handful of companies with the resources to train them. The open-source community continues making impressive progress, but the frontier keeps moving. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>

    <div class="post-navigation">
      <a href="/" class="back-to-blog">← Back to blog</a>
    </div>
  </div>
</article> 
      </div>
    </main><footer class="site-footer h-card">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h2 class="footer-heading">Sometimes Models Just Do Things</h2>
        <p>Where AI meets impatience, management consulting meets existential crisis, and prompts meet their match.</p>
      </div>

      <div class="footer-col footer-col-2">
        <h3>Categories</h3>
        <ul class="category-list">
          <li><a href="/categories/industry-developments">Industry Developments</a></li>
          <li><a href="/categories/experiments">Experiments</a></li>
          <li><a href="/categories/tools">Tools</a></li>
          <li><a href="/categories/tutorials">Tutorials</a></li>
        </ul>
      </div>
    </div>
    
    <div class="footer-bottom">
      <p>&copy; 2025 Ben Reeve</p>
    </div>
  </div>
</footer> </body>
</html> 