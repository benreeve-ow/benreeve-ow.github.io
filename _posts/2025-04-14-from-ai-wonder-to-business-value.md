---
layout: post
title: "From AI Wonder to Business Value: A Crawl-Walk-Run Framework"
date: 2025-04-14
categories: [tutorials]
author: Ben Reeve
image: /assets/images/posts/DC742323-486E-4137-AB10-64ADBDE19611.png
---

# From AI Wonder to Business Value: A Crawl-Walk-Run Framework

## Executive Summary

The first time most of us interact with advanced AI like ChatGPT, we get a little starry-eyed. The technology feels almost magical—writing children's stories, explaining complex regulations, generating functional code. This initial excitement inevitably leads to that fateful meeting where someone confidently declares: "We should be using this at work."

Fast forward a quarter or two, and the situation looks markedly different: half-finished AI pilots collecting digital dust, committees debating increasingly theoretical "use cases," and a growing realization that translating AI's impressive party tricks into business value is considerably more challenging than anticipated.

This implementation gap persists because organizations typically approach AI with three fundamental misunderstandings:

1. **Underestimating technical complexity**: Many leaders approach AI as though it were just another enterprise software deployment, failing to appreciate the nuances that determine success or failure.

2. **Overemphasizing technology over methodology**: Organizations become fixated on acquiring the latest AI models without developing systematic ways to evaluate, improve, and integrate these tools.

3. **Neglecting their data advantage**: Companies fail to recognize that their unique, proprietary data—not access to general AI models—represents their greatest potential source of competitive advantage.

To bridge this gap, organizations need a structured "Crawl-Walk-Run" approach:

- **Crawl: Systematic Benchmarking** – Develop objective measurements of AI performance in your specific business context
- **Walk: Methodical Improvement** – Implement structured evaluation frameworks and iterative refinement processes
- **Run: Strategic Training** – Leverage your organization's unique data assets and domain expertise to create customized AI capabilities (For the most advanced firms)

This framework shifts focus from technology fascination to value creation, transforming AI from an impressive technological curiosity into a sustainable source of competitive advantage.

## The AI Implementation Gap: Promise vs. Reality

Let's be honest: the gap between AI's theoretical potential and actual business impact remains substantial. The Stanford AI Index report shows that while 78% of organizations report using AI in some capacity (up from 55% the year before), "real business impact lags behind adoption." Most companies see only modest benefits, with typical revenue boosts from generative AI initiatives under 5%.

As one study's lead author diplomatically noted, "The narrative is that AI is everywhere... but the data shows it's harder to do." Translation: everyone's talking about it, few are doing it well, and even fewer are getting meaningful results.

Three fundamental shifts are occurring that organizations must understand:

**First, AI capabilities are transforming in fundamental ways.** We're not just getting more powerful chatbots. We're witnessing the emergence of systems like OpenAI's Deep Research agent—which achieves genuinely superhuman performance in specific domains. When Deep Research achieved 51.5% accuracy on finding obscure information that human researchers could only manage 29.2% on (even with two hours per question!), it wasn't just an incremental improvement. It was a fundamental shift in what's possible.

**Second, competitive advantage is shifting to proprietary data.** The major AI labs are pouring billions into three areas: more sophisticated machine learning techniques, bigger computational infrastructure, and better data. Unless you're planning to redirect your company's entire R&D budget into becoming the next OpenAI (please don't), your organization can really only compete in one area: your unique, proprietary data. As AI intelligence becomes increasingly commoditized, this proprietary data becomes your primary competitive moat.

**Third, implementation requires methodical progression, not technological leaps.** Organizations that successfully bridge the implementation gap follow a structured "Crawl-Walk-Run" approach that we'll explore in detail.

## Understanding AI: A Technical Primer for Business Leaders

To implement AI effectively, you need to develop genuine intuition about these technologies. You can't delegate this understanding any more than a CEO could delegate understanding their company's business model. This knowledge forms the foundation for all your strategic decisions about AI implementation.

The AI tools generating all the excitement today—particularly large language models (LLMs) like GPT-4.5, o1, or Claude 3.7 Sonnet—are built through a multi-stage development process. Each stage adds capabilities but also introduces complexities that affect how you should implement these technologies.

### The Four Components of Modern AI Systems

**1. The Base Model: Pattern Recognition at Scale**

At its foundation, a large language model is essentially a ridiculously sophisticated pattern recognition system trained on vast amounts of text. The result is impressively fluent text generation, but with a significant caveat: these raw base models don't inherently care about being helpful, accurate, or appropriate. They're pattern-matching machines, not aligned assistants.

**2. Supervised Fine-Tuning: Aligning with Human Preferences**

This is where the AI gets its manners. To make the base model more helpful and less chaotic, companies use supervised fine-tuning – essentially showing the model thousands of examples of desired behavior. After this training, the model becomes significantly more reliable as an assistant.

**3. Tool Integration: Extending Capabilities Beyond Language**

Even the most sophisticated language models have inherent limitations—they can't access real-time information, perform complex calculations reliably, or interact with external systems. To overcome these limitations, models can be integrated with tools such as web browsers, calculators, databases, and APIs.

**4. Reinforcement Learning: The Secret Sauce of AI Labs**

The final stage is where AI systems develop their seemingly magical capabilities. This is the secret sauce that major AI labs like OpenAI, Anthropic, and Google DeepMind are perfecting—and it's transforming what AI can accomplish. The most advanced reinforcement learning approaches now employ self-play techniques similar to those that revolutionized game-playing AI like AlphaZero.

What's revolutionary is how this approach has evolved beyond mastering games like chess and Go to systematically improving knowledge work. In this advanced form of reinforcement learning, the system creates questions with verifiable answers, then lets the model attempt multiple reasoning paths. The system identifies which approaches consistently lead to correct answers, and then trains the model on its own best attempts—something humans can't effectively judge for an LLM's internal reasoning.

This algorithmic refinement of reasoning strategies explains why commercial systems now perform tasks with an almost intuitive sense of how to approach problems across research, writing, coding, and analysis. Unlike systems that merely rely on human feedback of outputs, these models have internalized successful problem-solving patterns through thousands of iterations, creating the remarkable step-change in capability we're witnessing in the most advanced AI offerings.

Understanding these components explains why many organizations struggle with implementation. When trying to replicate capabilities of commercial AI systems internally, they focus solely on the model and basic tool integration, missing the sophisticated training that makes systems like o1 (with its trained reasoning capabilities) and Claude 3.7 Sonnet reliable.

## The Crawl-Walk-Run Framework: A Path to Implementation

Think of AI implementation as learning a complex new sport. You wouldn't start by attempting Olympic-level performances – you'd build capabilities methodically. Our Crawl-Walk-Run framework provides exactly this kind of structured progression.

### Crawl: Systematic Benchmarking

The first phase in our framework is to crawl—to establish objective measures of AI system performance in your specific context. "But benchmarking sounds boring," you might be thinking. "We need to move fast!" Trust me on this one: organizations that skip this step invariably find themselves moving very quickly in the wrong direction.

Think of benchmarking as your corporate GPS for AI implementation. Without it, you're essentially saying, "I know exactly where we need to go, I just have no idea where we currently are, and I don't want to check." That approach works about as well for AI as it does for actual navigation.

Leading AI research labs have developed sophisticated benchmarking approaches that you can adapt for your business. These approaches consist of several key elements:

1. **Comprehensive test sets**: Don't evaluate on just a handful of cherry-picked examples where the system happens to shine. That's like judging a basketball player based solely on their highlight reel.

2. **Objective metrics**: Establish clear, quantifiable measures of success. "It seems pretty good" isn't a metric. "It achieves 94% accuracy on priority classification with 98% consistency across evaluators" is a metric.

3. **Edge case coverage**: Deliberately include difficult or unusual cases. These often reveal critical weaknesses that would otherwise remain hidden until they cause problems in production.

4. **Consistency testing**: Evaluate how performance varies across multiple attempts or with slight variations in inputs. AI systems can be surprisingly brittle – working perfectly with one phrasing of a request and failing completely with a minor variation.

You don't need to reinvent these approaches from scratch. Organizations can either train internal teams to develop these benchmarking capabilities or work with experienced advisors who specialize in AI evaluation. The latter option can accelerate your implementation timeline significantly, as these specialists bring established frameworks and comparative insights from similar implementations. Whether you build internal capabilities or leverage external expertise depends on your timeline, budget, and strategic importance of AI to your organization.

### Walk: Methodical Testing and Improvement

Once you've established baseline measurements through benchmarking, you're ready to enter the "walk" phase. This is where you transform static evaluation into a dynamic improvement cycle. Instead of just asking "How good is our current system?" you start asking "How can we systematically make this better?"

This shift requires developing what AI research labs call "evals" – automated evaluation frameworks that provide consistent feedback on system changes. Think of it as setting up a gym for your AI system, with specific exercises designed to strengthen its weak points.

Several approaches from AI research prove particularly valuable during this phase:

**Prompt Engineering**: How you instruct an AI model dramatically affects its performance – sometimes to a surprising degree. It's not unlike dealing with a literal-minded but brilliant colleague who needs precisely worded instructions.

For example, many executives instinctively ask AI systems to "Summarize this data and give me the key insights" – a request that often yields disappointingly generic results. A more effective approach follows how these models actually process information: "First analyze the detailed patterns in this data. Then identify unusual trends or outliers. Finally, synthesize these findings into three key business implications."

**Tool Integration Refinement**: The ReAct framework (Reasoning and Acting) provides a valuable structure for refining how models interact with tools. It's the difference between:

1. A model programmed to use a calculator when it sees the word "math" (which works until someone asks about "arithmetic" instead)
2. A model that explicitly reasons: "This question involves multiplying large numbers. I should use the calculator tool for precision."

The second approach creates robustness that the first approach can never achieve. It's the difference between following rigid rules and developing genuine understanding of when tools are appropriate.

**Continuous Evaluation Cycles**: Implementing systematic testing requires establishing ongoing evaluation processes:

1. **Automated test suites**: Imagine having a virtual team that continuously checks your AI's homework, providing immediate feedback when it makes mistakes
2. **A/B testing**: The corporate equivalent of "Who wore it better?" for AI outputs, comparing performance of different approaches on identical inputs
3. **Behavioral testing**: Checking for consistency across variations
4. **Adversarial testing**: Intentionally challenging the system with difficult cases

### Run: Strategic Training and Deployment

Once you've established solid benchmarks and systematic improvement processes, you're ready to enter the "run" phase—scaling AI implementation through strategic training and deployment.

While the walking phase focuses on optimizing how you use existing AI models, the running phase explores how you can adapt models to better suit your specific needs. At this stage, you're no longer just a consumer of AI technology but an active participant in its development.

To be clear: this is an advanced stage that not every organization needs to reach. Many companies will derive substantial value just from the crawl and walk phases. Think of this as the Olympic level of AI implementation – impressive if you get there, but not necessary for many business purposes.

#### Leveraging Your Data Advantage

For most organizations, the most accessible and impactful approach to training focuses on proprietary data assets. While leading AI labs innovate along three dimensions—machine learning methods, compute infrastructure, and data—the first two require specialized expertise and billions in investment. Data, however, is an area where individual organizations often have unique advantages.

Consider this analogy: everyone can now buy roughly the same powerful cameras (AI models), but only you have access to your specific subjects (proprietary data). National Geographic photographers don't just have better cameras—they've invested heavily in reducing barriers to accessing unique subjects through specialized vehicles, local guides, and extensive planning. Similarly, leading organizations invest in making their proprietary data accessible, not just acquiring better AI models.

This raises an important question: when should you use commodity models versus investing in specialized solutions? The decision typically hinges on three factors:

1. **Task specificity**: For general tasks like summarizing public information or basic content creation, commodity models work well. For specialized industry tasks requiring proprietary knowledge or formats, customization delivers significantly higher value.

2. **Data advantage**: If your competitive advantage lies in unique data that commodity models haven't seen, investing in making this data accessible to AI systems (whether through training or retrieval techniques) can create substantial differentiation.

3. **Economic impact**: Customization makes sense when the business impact of improved performance justifies the investment. A 5% improvement in accuracy might be worth millions in some contexts and negligible in others.

#### Domain Adaptation Through Data Productization

One of the most powerful lessons from leading institutions is that effective AI implementation requires treating data as a product with clear ownership, quality standards, and user focus—not just as an IT asset managed by "those data people in the basement." This "data as a product" paradigm fundamentally changes how organizations prepare for AI.

When organizations implement domain-oriented data architectures, they typically designate domain teams as data product owners who deeply understand their data domains and can make informed decisions. This shift in ownership is crucial—it moves responsibility for data quality from a central IT team to those with the deepest domain knowledge.

Fine-tuning foundational models on these domain-specific data products can dramatically improve performance in specialized contexts. An AI trained on general internet data might understand what a "credit default swap" is in theory, but one fine-tuned on your firm's transaction history and analysis will understand how your specific organization evaluates and structures these instruments in practice.

#### Three Key Strategies for the Running Phase

**1. Data Product Development**

Create "AI-ready" data products with specific characteristics:

- **Contextually rich**: Include metadata and contextual information that helps AI systems understand the meaning and significance of the data.
- **Consistently formatted**: Standardize data formats and schemas to reduce the preprocessing burden.
- **Accessible through standardized interfaces**: Provide clear APIs or query interfaces that AI systems can use to retrieve data.
- **Well-documented**: Include detailed data dictionaries and relationship maps that explain what the data actually means.

**2. Feedback-Driven Improvement**

Those thumbs-up and thumbs-down buttons on ChatGPT aren't there for show. As AI systems interact with users in your organization, they generate valuable feedback data that can drive continuous improvement. This creates a virtuous cycle where the more your systems are used, the more feedback they receive, and the better they become at serving your specific needs.

**3. Synthetic Data Generation**

What if you need examples of rare but important scenarios? The evaluation frameworks developed during the walking phase can be extended to generate synthetic training data. By creating thousands of variations on key scenarios, you can train models on situations that might occur rarely in real data but are important to handle correctly.

## Practical Implementation Steps: Applying the Framework

Having established our Crawl-Walk-Run framework, let's examine how to implement each phase in practice. Rather than presenting these as separate steps, let's see how they connect directly to our framework.

### Crawl Phase Implementation

**1. Start with a Focused Pilot Project**
Rather than beginning with sweeping AI transformation plans, select a specific, well-defined pilot project with manageable scope and clear success criteria. A well-chosen pilot allows you to develop benchmarking capabilities and demonstrate value with controlled risk.

**2. Assemble a Cross-Functional Team**
Effective AI benchmarking requires diverse perspectives and skills. Your team should include data scientists or ML engineers, software developers, IT specialists, and crucially, domain experts who deeply understand the business processes being evaluated.

### Walk Phase Implementation

**3. Establish Domain-Oriented Data Ownership**
As you move into methodical improvement, establish clear ownership of data assets within the business domains that understand them best. This targeted approach avoids the common pitfall of trying to solve all data problems at once.

**4. Prioritize Strategic Data Assets**
Rather than attempting to improve all organizational data, identify specific data assets that represent proprietary knowledge, capture unique customer interactions, or document proprietary processes. These become your focus for systematic testing and improvement.

**5. Implement Gradual Deployment with Human Oversight**
As your testing shows improved performance, implement AI systems with appropriate human oversight through shadow mode, human-in-the-loop processes, and confidence thresholds. Crucially, create feedback loops that capture specific performance issues for ongoing refinement.

### Run Phase Implementation

**6. Establish Monitoring and Governance Protocols**
If you've ever watched a sci-fi movie where an AI system goes rogue, you've probably noticed one consistent plot element: nobody was really monitoring the thing until it was too late. Don't be that lab in the movie. As AI systems move into production, implement performance dashboards, automated alerts, and regular audits of system behavior.

**7. Systematically Record Performance Data for Future Training**
Most organizations evaluate AI output quality but fail to systematically record this data in a way that could be used for fine-tuning. Create structured processes to capture not just whether outputs were acceptable, but specifically what made them successful or problematic. This creates an invaluable dataset for future model improvements.

## Organizational Readiness and Change Management

Successfully implementing AI requires more than technical proficiency—it demands thoughtful attention to the human aspects of change. As AI capabilities rapidly evolve, many employees experience understandable anxiety about their future roles. Headlines about AI replacing jobs can create resistance that undermines implementation efforts regardless of their technical merit.

The systematic benchmarking approach described in the Crawl phase provides a powerful tool for addressing this anxiety. By establishing clear, objective measurements of AI performance in specific contexts, organizations help employees develop concrete understanding of:

1. **Current AI capabilities relative to human skills**: Proper benchmarking reveals both the impressive strengths and meaningful limitations of AI systems in your specific business context. This clarity helps replace vague fears ("the AI is coming for my job") with accurate understanding ("the AI handles routine claims well but struggles with complex negotiations").

2. **Areas where human expertise remains essential**: Performance measurements invariably reveal domains where human judgment, creativity, and contextual understanding continue to outperform AI systems. These aren't just feel-good statements to placate workers – they're objective findings from your own testing.

3. **Collaborative opportunities for human-AI teams**: Benchmarking often reveals that the highest performance comes from human-AI collaboration rather than either working alone. It's rarely "human vs. machine" but rather "humans with machines vs. humans without machines."

For example, when a financial services firm implemented systematic benchmarking of their AI investment analysis tools, they discovered that while the AI excelled at identifying patterns across thousands of data points, experienced analysts consistently outperformed it in evaluating management team capabilities and long-term strategic positioning.

The measurement frameworks established in the Crawl and Walk phases create natural opportunities for meaningful employee participation in AI development through performance evaluation roles, prompt engineering collaboration, and edge case identification. Organizations that actively involve employees in these processes find that participation transforms anxiety into engagement – it's the difference between having a robot assigned to your department versus helping design a tool that makes you more effective at the parts of your job you actually enjoy.

## Measuring Success and Connecting to ROI

To communicate the value of AI initiatives effectively, organizations should track both leading and lagging indicators:

**Leading Indicators (Early Signs of Progress)**:
- Improvement in benchmark performance scores
- Reduced human intervention rates over time
- Increased consistency of AI outputs
- Expansion of use cases handled reliably

**Lagging Indicators (Business Impact)**:
- Time saved for knowledge workers
- Reduced operational costs
- Increased throughput for key processes
- Improved customer satisfaction metrics
- Revenue generation from new AI-enabled capabilities

By establishing clear performance metrics early, organizations can draw direct connections between AI performance improvements and business value, creating meaningful ROI calculations that demonstrate the tangible value of their AI initiatives.

## Conclusion: Building a Path to AI Value

The implementation gap is real but surmountable. By adopting the Crawl-Walk-Run framework, organizations can develop AI capabilities that deliver genuine business value rather than just technological novelty.

There's a powerful parallel here to how organizations approach hiring talent. Many companies are essentially doing the tech equivalent of hiring smart people (purchasing AI tools), giving them vague objectives, providing little feedback when they go wrong, and then hiring someone else to do a slightly different job when disappointed. Just as this approach fails with human talent, it fails with AI. Success requires clear expectations, systematic feedback, and investment in growth—exactly what our framework provides.

**Crawl**: Systematic benchmarking establishes objective measures of AI performance in your specific context. This foundation prevents pursuing AI initiatives based on hype rather than demonstrated capability. It's rather like learning to assess potential romantic partners based on their actual behaviors rather than their carefully curated dating app profiles – a skill that saves considerable heartache down the road.

**Walk**: Methodical testing and improvement transforms static evaluation into a dynamic development cycle. This phase transforms AI from a mysterious black box into an engineered system that improves predictably. It's the corporate equivalent of turning "sometimes it works, sometimes it doesn't, who knows why?" into "we understand exactly why it works and can make it work better."

**Run**: Strategic training and deployment leverage your organization's unique data and domain expertise to create customized AI capabilities. Through domain adaptation, feedback-driven improvement, and synthetic data generation, you can develop AI systems that outperform general-purpose solutions in your specific context. This is where you stop buying off-the-rack AI and start getting something bespoke – tailored specifically to your organizational body shape, so to speak.

A final word of caution: AI is most successful when the outcome is objective and can be measured. If you can't build a testing process around a specific use case—if you can't clearly define when the AI is right or wrong—you're likely to struggle with implementation. In these situations, human processes may remain superior until your measurement capabilities improve.

The path from AI wonder to business value may be longer and more methodical than the breathless hype cycle suggests, but it leads to more sustainable, impactful results. It turns out that "move fast and break things" isn't actually the best approach when "things" includes "critical business processes" and "customer trust."

If there's one lesson that separates successful AI implementations from failed ones, it's this: the organizations that thrive don't chase AI capabilities for bragging rights at industry conferences. They build AI capabilities that address specific business needs with ruthless pragmatism and disciplined measurement.

In a world where everyone has access to increasingly powerful AI models, your proprietary data, domain expertise, and implementation methodology become your true competitive advantage. It's a bit like the democratization of smartphone cameras – when everyone has a great camera, the differentiator isn't the technology but what you uniquely point it at and how skillfully you compose the shot.

By focusing on the intersection of AI capabilities, your proprietary data, and your domain expertise, you can create solutions that not only deliver immediate value but also build sustainable competitive advantage in an increasingly AI-powered business landscape.

And isn't that what we're really after? Not AI that impresses at demos, but AI that delivers at scale – turning technological potential into business performance one carefully measured, methodically improved capability at a time. 