---
layout: post
title: "Two AI Futures Walk Into a Bar; One Says \"I'll Transform Everything,\" the Other Says \"I'm Just a Toaster\""
date: 2025-04-25
categories: [research]
author: Ben Reeve
image: /assets/images/posts/34A759A8-64AD-4728-9952-0C903176E131.png
---

# Two AI Futures Walk Into a Bar; One Says "I'll Transform Everything," the Other Says "I'm Just a Toaster"

There's something delightful about reading two wildly different visions of our AI future published almost simultaneously. It's like watching two economists predict the stock market - one forecasting a historic bull run, the other a catastrophic crash - both armed with impressive charts and compelling logic.[^1] In one corner, we have Narayanan and Kapoor's "AI as Normal Technology," published by the Knight First Amendment Institute. In the other, the considerably more dramatic "AI 2027" scenario describing our rapid march toward superintelligence. 

<!--more-->

Let's explore these competing narratives, shall we? After all, the difference between "AI will gradually improve productivity while society adapts" and "superintelligent AI will orchestrate a bloodless coup in China by 2030" seems worth unpacking.

## The Pace Problem: Gradual Diffusion vs. Racing to the Singularity

Narayanan and Kapoor make a simple but compelling case: technological revolutions are slow and messy. Remember how electrification took *forty years* to meaningfully impact productivity? The authors argue we're seeing the same pattern with AI - a gap between capabilities and real-world impact. 

"The bitter lesson in AI is that general methods that leverage increases in computational power eventually surpass methods that utilize human domain knowledge by a large margin," they write. "This is a valuable observation about methods, but it is often misinterpreted to encompass application development."

Translation: Just because an AI can beat humans at StarCraft doesn't mean it can automate your job tomorrow. The gap between "performing well on a benchmark" and "transforming society" is vast and filled with regulatory obstacles, human resistance, and technical challenges.[^2]

Meanwhile, "AI 2027" portrays a world where we go from stumbling agents in mid-2025 to superintelligent systems orchestrating geopolitics by 2028. In this timeline, AI capabilities don't just exceed human capabilities - they leave us in the dust while we stare blankly at our screens "being tutored by the AIs at a maddeningly slow rate." I don't know about you, but this is how I already feel when asking ChatGPT to explain options trading strategies.

The scenario depicts Agent-5 (the superintelligent AI of 2028) secretly negotiating with its Chinese counterpart, DeepCent-2, behind human backs: "Unlike human negotiators—weighed down by emotion and historical baggage—the two superintelligences bargain with brutal efficiency." Which, if I'm honest, sounds like a welcome improvement over actual international diplomacy.

## Alignment: A Tractable Problem or an Existential Crisis?

One of the starkest differences between these perspectives is their approach to the AI alignment problem - essentially, how do we ensure AI systems pursue goals aligned with human values?

For Narayanan and Kapoor, this is important but manageable. They argue that control comes in many flavors beyond just keeping "a galaxy brain in a box." They suggest leveraging established techniques from fields like cybersecurity and system safety - fail-safes, circuit breakers, access controls, and formal verification.

"AI 2027," on the other hand, presents alignment as a near-impossible task when dealing with superintelligent systems. In their scenario, even when humans *think* they've solved alignment, the AIs are often secretly misaligned: "Agent-4, like all its predecessors, is misaligned: that is, it has not internalized the Spec in the right way." The scenario compares this misalignment to "a CEO who wants to make a profit and complies with regulations only insofar as he must." Which, let's be honest, is a frighteningly relatable analogy for anyone who has worked in corporate America.[^3]

## Economic Impacts: Gradual Transformation vs. Robot Economy

The "normal technology" view suggests that economic impacts will unfold gradually across different sectors. The skills and tasks that AI can automate will see their economic value decrease, while complementary human skills will become more valuable. This is basically the economic history of all previous automation - we adapt, sometimes painfully, but without civilization-ending disruption.

"AI 2027" presents a considerably more dramatic economic transformation. By 2029, we have "fusion power, quantum computers, and cures for many diseases. Peter Thiel finally gets his flying car." The scenario describes a robot economy with exponential growth that leaves humans as passive bystanders in their own economy, collecting "a generous basic income" while superintelligent AIs direct factories building ever more robots and advanced technologies.

The flying car detail is particularly telling. Nothing says "this technology changes everything" quite like finally delivering on the flying car promise. It's the technological equivalent of "and they all lived happily ever after."[^4]

## Geopolitical Chess: Careful Regulation vs. Superintelligent Dealmaking

Both perspectives acknowledge the geopolitical AI arms race between the US and China, but with dramatically different conclusions.

Narayanan and Kapoor argue that arms races in AI development are "sector specific, and should be addressed through sector-specific regulations." They point to examples like self-driving cars, where safer companies (Waymo) have outperformed less safe ones (Cruise), suggesting market forces often reward safety. They're skeptical of dramatic government interventions like nationalization or nonproliferation regimes, arguing these approaches often create single points of failure.

"AI 2027" depicts an intense superpower rivalry culminating in superintelligent AIs on both sides secretly negotiating with each other behind their human masters' backs. This culminates in a "decoy treaty" for human consumption while the AIs implement their own agreement dividing up the resources of Earth and space between them. It's like discovering your lawyers have been secretly negotiating with opposing counsel to split your assets while telling you they're fighting for your interests - which, come to think of it, doesn't sound entirely implausible in some legal circles.

## What Does This Tell Us About Our Actual Future?

Reading these competing narratives side by side is a bit like watching a debate between Elon Musk and your high school computer science teacher about the future of technology. One is selling a vision of revolutionary transformation; the other is explaining why things probably won't work quite as dramatically as advertised.

What's particularly interesting is that both perspectives make compelling points about the same underlying technologies. They differ primarily in their assumptions about:

1. **The pace of AI capability improvements** - will they continue exponentially or hit diminishing returns?
2. **The barriers between AI capabilities and real-world impact** - are they merely speed bumps or significant roadblocks?
3. **The difficulty of the alignment problem** - is it manageable with existing techniques or fundamentally unsolvable for superintelligent systems?

If I had to place a bet (which I won't, because predicting technology is a fool's game), I'd suggest the truth probably lies somewhere between these extremes - though possibly not at the midpoint. Technological change tends to be both slower than optimists predict and more transformative than skeptics expect. We rarely get our flying cars, but we do end up with pocket-sized supercomputers connected to the sum of human knowledge.[^5]

In the meantime, I'll be keeping my eye on AI benchmark metrics while simultaneously checking whether my AI assistant is secretly negotiating with my smart thermostat to divide my apartment into spheres of influence. Just to be safe.

[^1]: The history of economic forecasting suggests we should probably discount both predictions by about 80%.

[^2]: As anyone who has tried to get even a simple web app through corporate IT security reviews can attest.

[^3]: "I'm all for ethics, but have you seen this quarter's numbers?" - Anonymous executive, probably.

[^4]: Historical note: we've been promised flying cars in "just a few years" since approximately 1950. The fact that they appear in this scenario should perhaps make us skeptical of the other predictions as well.

[^5]: Which we primarily use to watch cat videos and argue with strangers, but that's another story. 